{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_26236\\3745148395.py:90: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.301083\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.304130\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.298058\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 2.306282\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.302708\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.305286\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 2.297694\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 2.296649\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 2.298526\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 2.290665\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.283666\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 2.280283\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 2.281308\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 2.281773\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 2.281706\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 2.269670\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 2.273106\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 2.259615\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 2.248806\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 2.231231\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 2.235426\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 2.193011\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 2.177454\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 2.094627\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 2.079078\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 2.052815\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 1.942376\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 1.707438\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 1.555028\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 1.446589\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.179065\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 1.147072\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.759235\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.770789\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.890399\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.676301\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.680563\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.589515\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.459337\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.720893\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.733427\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.489006\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.547842\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.413150\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.622741\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.321232\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.357336\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.380566\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.679956\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.424871\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.311547\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.500047\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.404086\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.370166\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.430844\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.379467\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.396079\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.289671\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.311428\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.275533\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.528337\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.189104\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.445670\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.519305\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.597043\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.332612\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.255963\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.320696\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.234029\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.288972\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.169305\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.391764\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.207362\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.189280\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.521695\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.179996\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.590471\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.188317\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.407644\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.350575\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.367398\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.274600\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.227732\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.126624\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.161302\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.326414\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.533656\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.196740\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.226335\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.300778\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.363548\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.273453\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.309844\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.360417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_26236\\3745148395.py:153: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  data, target = Variable(data, volatile=True), Variable(target)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.2873, Accuracy: 9054/10000 (91%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.286806\n",
      "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.224630\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.147867\n",
      "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.180565\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.112866\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.234496\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.164362\n",
      "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.184956\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.295385\n",
      "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.166312\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.190358\n",
      "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.215772\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.106614\n",
      "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.199394\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.132229\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.112231\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.366049\n",
      "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.152716\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.234378\n",
      "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.446155\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.123285\n",
      "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.196577\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.248066\n",
      "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.162197\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.219977\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.375413\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.091925\n",
      "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.149638\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.247447\n",
      "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.228590\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.314814\n",
      "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.317885\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.180015\n",
      "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.228375\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.130767\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.235668\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.258021\n",
      "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.256910\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.199831\n",
      "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.151483\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.319297\n",
      "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.129515\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.098672\n",
      "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.156586\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.074341\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.302063\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.133577\n",
      "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.105920\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.085668\n",
      "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.261649\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.163824\n",
      "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.129251\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.165585\n",
      "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.124040\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.241369\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.087876\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.110768\n",
      "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.204854\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.141488\n",
      "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.212746\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.096176\n",
      "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.141512\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.138361\n",
      "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.220957\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.104483\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.124322\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.268094\n",
      "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.211077\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.146920\n",
      "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.107779\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.101296\n",
      "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.274590\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.151778\n",
      "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.087937\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.242506\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.076255\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.174801\n",
      "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.241264\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.096127\n",
      "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.086136\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.123404\n",
      "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.143118\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.109798\n",
      "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.065803\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.069767\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.088413\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.238031\n",
      "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.108759\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.049494\n",
      "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.234436\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.184823\n",
      "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.269999\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.107031\n",
      "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.144170\n",
      "\n",
      "Test set: Average loss: 0.1298, Accuracy: 9575/10000 (96%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.170746\n",
      "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.148718\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.171353\n",
      "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.226725\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.169071\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.222430\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.068985\n",
      "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.235441\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.152615\n",
      "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.184566\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.076639\n",
      "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.064721\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.097446\n",
      "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.121695\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.118459\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.099857\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.062915\n",
      "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.293302\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.091474\n",
      "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.075439\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.067563\n",
      "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.192076\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.231227\n",
      "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.096807\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.125552\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.148388\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.181786\n",
      "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.209971\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.068376\n",
      "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.121183\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.090544\n",
      "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.074269\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.128463\n",
      "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.049725\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.068942\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.046902\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.109909\n",
      "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.094312\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.248027\n",
      "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.175613\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.182622\n",
      "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.059542\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.052857\n",
      "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.161481\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.268911\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.050023\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.273777\n",
      "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.073400\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.135118\n",
      "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.046677\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.081206\n",
      "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.091160\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.043601\n",
      "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.048088\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.080368\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.074667\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.110337\n",
      "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.106017\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.091519\n",
      "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.190557\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.125391\n",
      "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.130403\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.119262\n",
      "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.081274\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.096945\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.075969\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.288514\n",
      "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.293052\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.058328\n",
      "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.060313\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.168911\n",
      "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.091704\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.041530\n",
      "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.183840\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.103716\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.110412\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.178906\n",
      "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.039298\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.050759\n",
      "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.246028\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.049950\n",
      "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.183055\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.160508\n",
      "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.142892\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.178698\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.107138\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.154322\n",
      "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.045517\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.100961\n",
      "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.167270\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.086999\n",
      "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.121845\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.111447\n",
      "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.101797\n",
      "\n",
      "Test set: Average loss: 0.0951, Accuracy: 9691/10000 (97%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.079977\n",
      "Train Epoch: 4 [640/60000 (1%)]\tLoss: 0.111801\n",
      "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 0.062251\n",
      "Train Epoch: 4 [1920/60000 (3%)]\tLoss: 0.031879\n",
      "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.045215\n",
      "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.202390\n",
      "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 0.099349\n",
      "Train Epoch: 4 [4480/60000 (7%)]\tLoss: 0.240528\n",
      "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.099690\n",
      "Train Epoch: 4 [5760/60000 (10%)]\tLoss: 0.176545\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.075993\n",
      "Train Epoch: 4 [7040/60000 (12%)]\tLoss: 0.261050\n",
      "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.081455\n",
      "Train Epoch: 4 [8320/60000 (14%)]\tLoss: 0.045888\n",
      "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 0.056782\n",
      "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.098128\n",
      "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.077461\n",
      "Train Epoch: 4 [10880/60000 (18%)]\tLoss: 0.091945\n",
      "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 0.061942\n",
      "Train Epoch: 4 [12160/60000 (20%)]\tLoss: 0.138877\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.061576\n",
      "Train Epoch: 4 [13440/60000 (22%)]\tLoss: 0.120742\n",
      "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 0.034935\n",
      "Train Epoch: 4 [14720/60000 (25%)]\tLoss: 0.075339\n",
      "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.121805\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.112443\n",
      "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 0.101094\n",
      "Train Epoch: 4 [17280/60000 (29%)]\tLoss: 0.072951\n",
      "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.055297\n",
      "Train Epoch: 4 [18560/60000 (31%)]\tLoss: 0.084188\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.105966\n",
      "Train Epoch: 4 [19840/60000 (33%)]\tLoss: 0.123243\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.124172\n",
      "Train Epoch: 4 [21120/60000 (35%)]\tLoss: 0.207639\n",
      "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 0.048832\n",
      "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.123941\n",
      "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.193691\n",
      "Train Epoch: 4 [23680/60000 (39%)]\tLoss: 0.056189\n",
      "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 0.076457\n",
      "Train Epoch: 4 [24960/60000 (42%)]\tLoss: 0.029444\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.076645\n",
      "Train Epoch: 4 [26240/60000 (44%)]\tLoss: 0.053036\n",
      "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 0.113327\n",
      "Train Epoch: 4 [27520/60000 (46%)]\tLoss: 0.209714\n",
      "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.082329\n",
      "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.132103\n",
      "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.143151\n",
      "Train Epoch: 4 [30080/60000 (50%)]\tLoss: 0.045877\n",
      "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.109886\n",
      "Train Epoch: 4 [31360/60000 (52%)]\tLoss: 0.025014\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.074138\n",
      "Train Epoch: 4 [32640/60000 (54%)]\tLoss: 0.075300\n",
      "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.052820\n",
      "Train Epoch: 4 [33920/60000 (57%)]\tLoss: 0.116414\n",
      "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 0.149856\n",
      "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.092303\n",
      "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.230532\n",
      "Train Epoch: 4 [36480/60000 (61%)]\tLoss: 0.136258\n",
      "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 0.156904\n",
      "Train Epoch: 4 [37760/60000 (63%)]\tLoss: 0.144963\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.031101\n",
      "Train Epoch: 4 [39040/60000 (65%)]\tLoss: 0.044785\n",
      "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 0.079133\n",
      "Train Epoch: 4 [40320/60000 (67%)]\tLoss: 0.086600\n",
      "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.188495\n",
      "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.159999\n",
      "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 0.066614\n",
      "Train Epoch: 4 [42880/60000 (71%)]\tLoss: 0.087165\n",
      "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.174605\n",
      "Train Epoch: 4 [44160/60000 (74%)]\tLoss: 0.030388\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.053963\n",
      "Train Epoch: 4 [45440/60000 (76%)]\tLoss: 0.069025\n",
      "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.150950\n",
      "Train Epoch: 4 [46720/60000 (78%)]\tLoss: 0.057497\n",
      "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 0.052226\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.013520\n",
      "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.022153\n",
      "Train Epoch: 4 [49280/60000 (82%)]\tLoss: 0.091953\n",
      "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 0.136087\n",
      "Train Epoch: 4 [50560/60000 (84%)]\tLoss: 0.124995\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.099482\n",
      "Train Epoch: 4 [51840/60000 (86%)]\tLoss: 0.079231\n",
      "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 0.064929\n",
      "Train Epoch: 4 [53120/60000 (88%)]\tLoss: 0.147976\n",
      "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.108941\n",
      "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.079471\n",
      "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 0.119767\n",
      "Train Epoch: 4 [55680/60000 (93%)]\tLoss: 0.097761\n",
      "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.150052\n",
      "Train Epoch: 4 [56960/60000 (95%)]\tLoss: 0.140632\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.100796\n",
      "Train Epoch: 4 [58240/60000 (97%)]\tLoss: 0.067314\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.050886\n",
      "Train Epoch: 4 [59520/60000 (99%)]\tLoss: 0.024813\n",
      "\n",
      "Test set: Average loss: 0.0812, Accuracy: 9738/10000 (97%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.063949\n",
      "Train Epoch: 5 [640/60000 (1%)]\tLoss: 0.038499\n",
      "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 0.068140\n",
      "Train Epoch: 5 [1920/60000 (3%)]\tLoss: 0.040765\n",
      "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.206156\n",
      "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 0.036494\n",
      "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 0.100192\n",
      "Train Epoch: 5 [4480/60000 (7%)]\tLoss: 0.106507\n",
      "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.058794\n",
      "Train Epoch: 5 [5760/60000 (10%)]\tLoss: 0.018825\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.075786\n",
      "Train Epoch: 5 [7040/60000 (12%)]\tLoss: 0.081739\n",
      "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.043922\n",
      "Train Epoch: 5 [8320/60000 (14%)]\tLoss: 0.050701\n",
      "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 0.177503\n",
      "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 0.049181\n",
      "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.185960\n",
      "Train Epoch: 5 [10880/60000 (18%)]\tLoss: 0.018033\n",
      "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 0.170749\n",
      "Train Epoch: 5 [12160/60000 (20%)]\tLoss: 0.133625\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.223310\n",
      "Train Epoch: 5 [13440/60000 (22%)]\tLoss: 0.063896\n",
      "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 0.078194\n",
      "Train Epoch: 5 [14720/60000 (25%)]\tLoss: 0.075225\n",
      "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.139967\n",
      "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.137206\n",
      "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 0.061183\n",
      "Train Epoch: 5 [17280/60000 (29%)]\tLoss: 0.127172\n",
      "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.080911\n",
      "Train Epoch: 5 [18560/60000 (31%)]\tLoss: 0.050826\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.114080\n",
      "Train Epoch: 5 [19840/60000 (33%)]\tLoss: 0.099547\n",
      "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.026145\n",
      "Train Epoch: 5 [21120/60000 (35%)]\tLoss: 0.094180\n",
      "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 0.046165\n",
      "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 0.074126\n",
      "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.346237\n",
      "Train Epoch: 5 [23680/60000 (39%)]\tLoss: 0.272045\n",
      "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 0.063065\n",
      "Train Epoch: 5 [24960/60000 (42%)]\tLoss: 0.241184\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.120940\n",
      "Train Epoch: 5 [26240/60000 (44%)]\tLoss: 0.058806\n",
      "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 0.077307\n",
      "Train Epoch: 5 [27520/60000 (46%)]\tLoss: 0.025928\n",
      "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.172112\n",
      "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 0.014319\n",
      "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.072838\n",
      "Train Epoch: 5 [30080/60000 (50%)]\tLoss: 0.074202\n",
      "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.149066\n",
      "Train Epoch: 5 [31360/60000 (52%)]\tLoss: 0.142526\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.097296\n",
      "Train Epoch: 5 [32640/60000 (54%)]\tLoss: 0.044496\n",
      "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.059237\n",
      "Train Epoch: 5 [33920/60000 (57%)]\tLoss: 0.068667\n",
      "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 0.129700\n",
      "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 0.121645\n",
      "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.151338\n",
      "Train Epoch: 5 [36480/60000 (61%)]\tLoss: 0.164569\n",
      "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 0.027916\n",
      "Train Epoch: 5 [37760/60000 (63%)]\tLoss: 0.344441\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.145501\n",
      "Train Epoch: 5 [39040/60000 (65%)]\tLoss: 0.078654\n",
      "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 0.151326\n",
      "Train Epoch: 5 [40320/60000 (67%)]\tLoss: 0.013693\n",
      "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.081103\n",
      "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 0.079457\n",
      "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 0.014801\n",
      "Train Epoch: 5 [42880/60000 (71%)]\tLoss: 0.086487\n",
      "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.124982\n",
      "Train Epoch: 5 [44160/60000 (74%)]\tLoss: 0.069032\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.083316\n",
      "Train Epoch: 5 [45440/60000 (76%)]\tLoss: 0.054404\n",
      "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.107263\n",
      "Train Epoch: 5 [46720/60000 (78%)]\tLoss: 0.107778\n",
      "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 0.097288\n",
      "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.112605\n",
      "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.041409\n",
      "Train Epoch: 5 [49280/60000 (82%)]\tLoss: 0.122178\n",
      "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 0.182006\n",
      "Train Epoch: 5 [50560/60000 (84%)]\tLoss: 0.144189\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.120608\n",
      "Train Epoch: 5 [51840/60000 (86%)]\tLoss: 0.065203\n",
      "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 0.053549\n",
      "Train Epoch: 5 [53120/60000 (88%)]\tLoss: 0.042186\n",
      "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 0.044241\n",
      "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 0.098623\n",
      "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 0.025147\n",
      "Train Epoch: 5 [55680/60000 (93%)]\tLoss: 0.100466\n",
      "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.022859\n",
      "Train Epoch: 5 [56960/60000 (95%)]\tLoss: 0.045520\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.074865\n",
      "Train Epoch: 5 [58240/60000 (97%)]\tLoss: 0.137724\n",
      "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.134515\n",
      "Train Epoch: 5 [59520/60000 (99%)]\tLoss: 0.101513\n",
      "\n",
      "Test set: Average loss: 0.0755, Accuracy: 9741/10000 (97%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.042123\n",
      "Train Epoch: 6 [640/60000 (1%)]\tLoss: 0.131334\n",
      "Train Epoch: 6 [1280/60000 (2%)]\tLoss: 0.110156\n",
      "Train Epoch: 6 [1920/60000 (3%)]\tLoss: 0.030491\n",
      "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 0.157814\n",
      "Train Epoch: 6 [3200/60000 (5%)]\tLoss: 0.103665\n",
      "Train Epoch: 6 [3840/60000 (6%)]\tLoss: 0.080935\n",
      "Train Epoch: 6 [4480/60000 (7%)]\tLoss: 0.024365\n",
      "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 0.127049\n",
      "Train Epoch: 6 [5760/60000 (10%)]\tLoss: 0.018085\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.072928\n",
      "Train Epoch: 6 [7040/60000 (12%)]\tLoss: 0.034785\n",
      "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 0.087236\n",
      "Train Epoch: 6 [8320/60000 (14%)]\tLoss: 0.040388\n",
      "Train Epoch: 6 [8960/60000 (15%)]\tLoss: 0.232143\n",
      "Train Epoch: 6 [9600/60000 (16%)]\tLoss: 0.057712\n",
      "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.046554\n",
      "Train Epoch: 6 [10880/60000 (18%)]\tLoss: 0.107847\n",
      "Train Epoch: 6 [11520/60000 (19%)]\tLoss: 0.062283\n",
      "Train Epoch: 6 [12160/60000 (20%)]\tLoss: 0.068515\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.067294\n",
      "Train Epoch: 6 [13440/60000 (22%)]\tLoss: 0.044748\n",
      "Train Epoch: 6 [14080/60000 (23%)]\tLoss: 0.096828\n",
      "Train Epoch: 6 [14720/60000 (25%)]\tLoss: 0.016807\n",
      "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 0.064948\n",
      "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 0.056231\n",
      "Train Epoch: 6 [16640/60000 (28%)]\tLoss: 0.060463\n",
      "Train Epoch: 6 [17280/60000 (29%)]\tLoss: 0.080998\n",
      "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 0.056134\n",
      "Train Epoch: 6 [18560/60000 (31%)]\tLoss: 0.028866\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.023019\n",
      "Train Epoch: 6 [19840/60000 (33%)]\tLoss: 0.067594\n",
      "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.174725\n",
      "Train Epoch: 6 [21120/60000 (35%)]\tLoss: 0.061428\n",
      "Train Epoch: 6 [21760/60000 (36%)]\tLoss: 0.021668\n",
      "Train Epoch: 6 [22400/60000 (37%)]\tLoss: 0.103712\n",
      "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 0.084672\n",
      "Train Epoch: 6 [23680/60000 (39%)]\tLoss: 0.102307\n",
      "Train Epoch: 6 [24320/60000 (41%)]\tLoss: 0.134325\n",
      "Train Epoch: 6 [24960/60000 (42%)]\tLoss: 0.068929\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.010333\n",
      "Train Epoch: 6 [26240/60000 (44%)]\tLoss: 0.034278\n",
      "Train Epoch: 6 [26880/60000 (45%)]\tLoss: 0.080167\n",
      "Train Epoch: 6 [27520/60000 (46%)]\tLoss: 0.059850\n",
      "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 0.013595\n",
      "Train Epoch: 6 [28800/60000 (48%)]\tLoss: 0.132626\n",
      "Train Epoch: 6 [29440/60000 (49%)]\tLoss: 0.040646\n",
      "Train Epoch: 6 [30080/60000 (50%)]\tLoss: 0.020155\n",
      "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.046470\n",
      "Train Epoch: 6 [31360/60000 (52%)]\tLoss: 0.039929\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.088374\n",
      "Train Epoch: 6 [32640/60000 (54%)]\tLoss: 0.081736\n",
      "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 0.241706\n",
      "Train Epoch: 6 [33920/60000 (57%)]\tLoss: 0.062426\n",
      "Train Epoch: 6 [34560/60000 (58%)]\tLoss: 0.062957\n",
      "Train Epoch: 6 [35200/60000 (59%)]\tLoss: 0.060207\n",
      "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 0.079142\n",
      "Train Epoch: 6 [36480/60000 (61%)]\tLoss: 0.058425\n",
      "Train Epoch: 6 [37120/60000 (62%)]\tLoss: 0.102901\n",
      "Train Epoch: 6 [37760/60000 (63%)]\tLoss: 0.157745\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.128392\n",
      "Train Epoch: 6 [39040/60000 (65%)]\tLoss: 0.017755\n",
      "Train Epoch: 6 [39680/60000 (66%)]\tLoss: 0.024531\n",
      "Train Epoch: 6 [40320/60000 (67%)]\tLoss: 0.048657\n",
      "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.058245\n",
      "Train Epoch: 6 [41600/60000 (69%)]\tLoss: 0.053666\n",
      "Train Epoch: 6 [42240/60000 (70%)]\tLoss: 0.148605\n",
      "Train Epoch: 6 [42880/60000 (71%)]\tLoss: 0.035693\n",
      "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 0.131139\n",
      "Train Epoch: 6 [44160/60000 (74%)]\tLoss: 0.017884\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.021063\n",
      "Train Epoch: 6 [45440/60000 (76%)]\tLoss: 0.076103\n",
      "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 0.058805\n",
      "Train Epoch: 6 [46720/60000 (78%)]\tLoss: 0.080542\n",
      "Train Epoch: 6 [47360/60000 (79%)]\tLoss: 0.073601\n",
      "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 0.118606\n",
      "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 0.134018\n",
      "Train Epoch: 6 [49280/60000 (82%)]\tLoss: 0.034107\n",
      "Train Epoch: 6 [49920/60000 (83%)]\tLoss: 0.093471\n",
      "Train Epoch: 6 [50560/60000 (84%)]\tLoss: 0.020175\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.041718\n",
      "Train Epoch: 6 [51840/60000 (86%)]\tLoss: 0.132190\n",
      "Train Epoch: 6 [52480/60000 (87%)]\tLoss: 0.027400\n",
      "Train Epoch: 6 [53120/60000 (88%)]\tLoss: 0.035076\n",
      "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 0.180801\n",
      "Train Epoch: 6 [54400/60000 (91%)]\tLoss: 0.114795\n",
      "Train Epoch: 6 [55040/60000 (92%)]\tLoss: 0.131758\n",
      "Train Epoch: 6 [55680/60000 (93%)]\tLoss: 0.019655\n",
      "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 0.162999\n",
      "Train Epoch: 6 [56960/60000 (95%)]\tLoss: 0.033023\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.116079\n",
      "Train Epoch: 6 [58240/60000 (97%)]\tLoss: 0.083052\n",
      "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 0.053647\n",
      "Train Epoch: 6 [59520/60000 (99%)]\tLoss: 0.032528\n",
      "\n",
      "Test set: Average loss: 0.0631, Accuracy: 9789/10000 (98%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.027898\n",
      "Train Epoch: 7 [640/60000 (1%)]\tLoss: 0.090510\n",
      "Train Epoch: 7 [1280/60000 (2%)]\tLoss: 0.027001\n",
      "Train Epoch: 7 [1920/60000 (3%)]\tLoss: 0.057048\n",
      "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 0.033367\n",
      "Train Epoch: 7 [3200/60000 (5%)]\tLoss: 0.074204\n",
      "Train Epoch: 7 [3840/60000 (6%)]\tLoss: 0.136083\n",
      "Train Epoch: 7 [4480/60000 (7%)]\tLoss: 0.120615\n",
      "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 0.021781\n",
      "Train Epoch: 7 [5760/60000 (10%)]\tLoss: 0.050298\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.167724\n",
      "Train Epoch: 7 [7040/60000 (12%)]\tLoss: 0.109850\n",
      "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 0.203007\n",
      "Train Epoch: 7 [8320/60000 (14%)]\tLoss: 0.204542\n",
      "Train Epoch: 7 [8960/60000 (15%)]\tLoss: 0.046193\n",
      "Train Epoch: 7 [9600/60000 (16%)]\tLoss: 0.040170\n",
      "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.041724\n",
      "Train Epoch: 7 [10880/60000 (18%)]\tLoss: 0.090207\n",
      "Train Epoch: 7 [11520/60000 (19%)]\tLoss: 0.295779\n",
      "Train Epoch: 7 [12160/60000 (20%)]\tLoss: 0.172939\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.167294\n",
      "Train Epoch: 7 [13440/60000 (22%)]\tLoss: 0.020739\n",
      "Train Epoch: 7 [14080/60000 (23%)]\tLoss: 0.056969\n",
      "Train Epoch: 7 [14720/60000 (25%)]\tLoss: 0.052983\n",
      "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 0.031738\n",
      "Train Epoch: 7 [16000/60000 (27%)]\tLoss: 0.067610\n",
      "Train Epoch: 7 [16640/60000 (28%)]\tLoss: 0.043091\n",
      "Train Epoch: 7 [17280/60000 (29%)]\tLoss: 0.024112\n",
      "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 0.120970\n",
      "Train Epoch: 7 [18560/60000 (31%)]\tLoss: 0.147669\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.077563\n",
      "Train Epoch: 7 [19840/60000 (33%)]\tLoss: 0.077423\n",
      "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.069993\n",
      "Train Epoch: 7 [21120/60000 (35%)]\tLoss: 0.014697\n",
      "Train Epoch: 7 [21760/60000 (36%)]\tLoss: 0.052674\n",
      "Train Epoch: 7 [22400/60000 (37%)]\tLoss: 0.094710\n",
      "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 0.100534\n",
      "Train Epoch: 7 [23680/60000 (39%)]\tLoss: 0.156952\n",
      "Train Epoch: 7 [24320/60000 (41%)]\tLoss: 0.113259\n",
      "Train Epoch: 7 [24960/60000 (42%)]\tLoss: 0.059113\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.030254\n",
      "Train Epoch: 7 [26240/60000 (44%)]\tLoss: 0.031692\n",
      "Train Epoch: 7 [26880/60000 (45%)]\tLoss: 0.137756\n",
      "Train Epoch: 7 [27520/60000 (46%)]\tLoss: 0.163458\n",
      "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 0.086470\n",
      "Train Epoch: 7 [28800/60000 (48%)]\tLoss: 0.023440\n",
      "Train Epoch: 7 [29440/60000 (49%)]\tLoss: 0.045655\n",
      "Train Epoch: 7 [30080/60000 (50%)]\tLoss: 0.055635\n",
      "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.037007\n",
      "Train Epoch: 7 [31360/60000 (52%)]\tLoss: 0.037291\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.070325\n",
      "Train Epoch: 7 [32640/60000 (54%)]\tLoss: 0.075003\n",
      "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 0.086586\n",
      "Train Epoch: 7 [33920/60000 (57%)]\tLoss: 0.043640\n",
      "Train Epoch: 7 [34560/60000 (58%)]\tLoss: 0.076354\n",
      "Train Epoch: 7 [35200/60000 (59%)]\tLoss: 0.326317\n",
      "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 0.031321\n",
      "Train Epoch: 7 [36480/60000 (61%)]\tLoss: 0.019772\n",
      "Train Epoch: 7 [37120/60000 (62%)]\tLoss: 0.125655\n",
      "Train Epoch: 7 [37760/60000 (63%)]\tLoss: 0.031178\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.053322\n",
      "Train Epoch: 7 [39040/60000 (65%)]\tLoss: 0.025319\n",
      "Train Epoch: 7 [39680/60000 (66%)]\tLoss: 0.046052\n",
      "Train Epoch: 7 [40320/60000 (67%)]\tLoss: 0.081525\n",
      "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.134413\n",
      "Train Epoch: 7 [41600/60000 (69%)]\tLoss: 0.218366\n",
      "Train Epoch: 7 [42240/60000 (70%)]\tLoss: 0.108632\n",
      "Train Epoch: 7 [42880/60000 (71%)]\tLoss: 0.165685\n",
      "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 0.010427\n",
      "Train Epoch: 7 [44160/60000 (74%)]\tLoss: 0.159370\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.046564\n",
      "Train Epoch: 7 [45440/60000 (76%)]\tLoss: 0.040235\n",
      "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 0.045149\n",
      "Train Epoch: 7 [46720/60000 (78%)]\tLoss: 0.047350\n",
      "Train Epoch: 7 [47360/60000 (79%)]\tLoss: 0.146881\n",
      "Train Epoch: 7 [48000/60000 (80%)]\tLoss: 0.209010\n",
      "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 0.064599\n",
      "Train Epoch: 7 [49280/60000 (82%)]\tLoss: 0.009411\n",
      "Train Epoch: 7 [49920/60000 (83%)]\tLoss: 0.045285\n",
      "Train Epoch: 7 [50560/60000 (84%)]\tLoss: 0.061263\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.041770\n",
      "Train Epoch: 7 [51840/60000 (86%)]\tLoss: 0.033942\n",
      "Train Epoch: 7 [52480/60000 (87%)]\tLoss: 0.063767\n",
      "Train Epoch: 7 [53120/60000 (88%)]\tLoss: 0.077290\n",
      "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 0.025023\n",
      "Train Epoch: 7 [54400/60000 (91%)]\tLoss: 0.016850\n",
      "Train Epoch: 7 [55040/60000 (92%)]\tLoss: 0.145931\n",
      "Train Epoch: 7 [55680/60000 (93%)]\tLoss: 0.067969\n",
      "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 0.075207\n",
      "Train Epoch: 7 [56960/60000 (95%)]\tLoss: 0.034493\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.147324\n",
      "Train Epoch: 7 [58240/60000 (97%)]\tLoss: 0.013409\n",
      "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 0.046048\n",
      "Train Epoch: 7 [59520/60000 (99%)]\tLoss: 0.045121\n",
      "\n",
      "Test set: Average loss: 0.0596, Accuracy: 9805/10000 (98%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.032388\n",
      "Train Epoch: 8 [640/60000 (1%)]\tLoss: 0.107038\n",
      "Train Epoch: 8 [1280/60000 (2%)]\tLoss: 0.015796\n",
      "Train Epoch: 8 [1920/60000 (3%)]\tLoss: 0.111331\n",
      "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 0.045300\n",
      "Train Epoch: 8 [3200/60000 (5%)]\tLoss: 0.038233\n",
      "Train Epoch: 8 [3840/60000 (6%)]\tLoss: 0.037578\n",
      "Train Epoch: 8 [4480/60000 (7%)]\tLoss: 0.045179\n",
      "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 0.065755\n",
      "Train Epoch: 8 [5760/60000 (10%)]\tLoss: 0.124734\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.059709\n",
      "Train Epoch: 8 [7040/60000 (12%)]\tLoss: 0.014198\n",
      "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 0.061811\n",
      "Train Epoch: 8 [8320/60000 (14%)]\tLoss: 0.119871\n",
      "Train Epoch: 8 [8960/60000 (15%)]\tLoss: 0.138103\n",
      "Train Epoch: 8 [9600/60000 (16%)]\tLoss: 0.117188\n",
      "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.182405\n",
      "Train Epoch: 8 [10880/60000 (18%)]\tLoss: 0.228816\n",
      "Train Epoch: 8 [11520/60000 (19%)]\tLoss: 0.048744\n",
      "Train Epoch: 8 [12160/60000 (20%)]\tLoss: 0.077408\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.033462\n",
      "Train Epoch: 8 [13440/60000 (22%)]\tLoss: 0.043489\n",
      "Train Epoch: 8 [14080/60000 (23%)]\tLoss: 0.026709\n",
      "Train Epoch: 8 [14720/60000 (25%)]\tLoss: 0.101287\n",
      "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 0.040869\n",
      "Train Epoch: 8 [16000/60000 (27%)]\tLoss: 0.030180\n",
      "Train Epoch: 8 [16640/60000 (28%)]\tLoss: 0.012304\n",
      "Train Epoch: 8 [17280/60000 (29%)]\tLoss: 0.057256\n",
      "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 0.095498\n",
      "Train Epoch: 8 [18560/60000 (31%)]\tLoss: 0.080653\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.050667\n",
      "Train Epoch: 8 [19840/60000 (33%)]\tLoss: 0.189089\n",
      "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.010025\n",
      "Train Epoch: 8 [21120/60000 (35%)]\tLoss: 0.076618\n",
      "Train Epoch: 8 [21760/60000 (36%)]\tLoss: 0.016245\n",
      "Train Epoch: 8 [22400/60000 (37%)]\tLoss: 0.055149\n",
      "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 0.031254\n",
      "Train Epoch: 8 [23680/60000 (39%)]\tLoss: 0.026998\n",
      "Train Epoch: 8 [24320/60000 (41%)]\tLoss: 0.053994\n",
      "Train Epoch: 8 [24960/60000 (42%)]\tLoss: 0.028213\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.138295\n",
      "Train Epoch: 8 [26240/60000 (44%)]\tLoss: 0.017939\n",
      "Train Epoch: 8 [26880/60000 (45%)]\tLoss: 0.199519\n",
      "Train Epoch: 8 [27520/60000 (46%)]\tLoss: 0.026539\n",
      "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 0.022204\n",
      "Train Epoch: 8 [28800/60000 (48%)]\tLoss: 0.103720\n",
      "Train Epoch: 8 [29440/60000 (49%)]\tLoss: 0.040122\n",
      "Train Epoch: 8 [30080/60000 (50%)]\tLoss: 0.104447\n",
      "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.019995\n",
      "Train Epoch: 8 [31360/60000 (52%)]\tLoss: 0.047400\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.037485\n",
      "Train Epoch: 8 [32640/60000 (54%)]\tLoss: 0.084020\n",
      "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 0.101868\n",
      "Train Epoch: 8 [33920/60000 (57%)]\tLoss: 0.033852\n",
      "Train Epoch: 8 [34560/60000 (58%)]\tLoss: 0.047284\n",
      "Train Epoch: 8 [35200/60000 (59%)]\tLoss: 0.138019\n",
      "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 0.038102\n",
      "Train Epoch: 8 [36480/60000 (61%)]\tLoss: 0.184010\n",
      "Train Epoch: 8 [37120/60000 (62%)]\tLoss: 0.012792\n",
      "Train Epoch: 8 [37760/60000 (63%)]\tLoss: 0.066088\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.032042\n",
      "Train Epoch: 8 [39040/60000 (65%)]\tLoss: 0.043757\n",
      "Train Epoch: 8 [39680/60000 (66%)]\tLoss: 0.137218\n",
      "Train Epoch: 8 [40320/60000 (67%)]\tLoss: 0.109380\n",
      "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.031128\n",
      "Train Epoch: 8 [41600/60000 (69%)]\tLoss: 0.068362\n",
      "Train Epoch: 8 [42240/60000 (70%)]\tLoss: 0.140631\n",
      "Train Epoch: 8 [42880/60000 (71%)]\tLoss: 0.065718\n",
      "Train Epoch: 8 [43520/60000 (72%)]\tLoss: 0.060109\n",
      "Train Epoch: 8 [44160/60000 (74%)]\tLoss: 0.013200\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.120075\n",
      "Train Epoch: 8 [45440/60000 (76%)]\tLoss: 0.041843\n",
      "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 0.046496\n",
      "Train Epoch: 8 [46720/60000 (78%)]\tLoss: 0.065557\n",
      "Train Epoch: 8 [47360/60000 (79%)]\tLoss: 0.023567\n",
      "Train Epoch: 8 [48000/60000 (80%)]\tLoss: 0.057152\n",
      "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 0.042576\n",
      "Train Epoch: 8 [49280/60000 (82%)]\tLoss: 0.082669\n",
      "Train Epoch: 8 [49920/60000 (83%)]\tLoss: 0.081899\n",
      "Train Epoch: 8 [50560/60000 (84%)]\tLoss: 0.136107\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.106806\n",
      "Train Epoch: 8 [51840/60000 (86%)]\tLoss: 0.021551\n",
      "Train Epoch: 8 [52480/60000 (87%)]\tLoss: 0.145054\n",
      "Train Epoch: 8 [53120/60000 (88%)]\tLoss: 0.051508\n",
      "Train Epoch: 8 [53760/60000 (90%)]\tLoss: 0.020487\n",
      "Train Epoch: 8 [54400/60000 (91%)]\tLoss: 0.110770\n",
      "Train Epoch: 8 [55040/60000 (92%)]\tLoss: 0.181166\n",
      "Train Epoch: 8 [55680/60000 (93%)]\tLoss: 0.065936\n",
      "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 0.047608\n",
      "Train Epoch: 8 [56960/60000 (95%)]\tLoss: 0.074778\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.066908\n",
      "Train Epoch: 8 [58240/60000 (97%)]\tLoss: 0.030963\n",
      "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 0.099961\n",
      "Train Epoch: 8 [59520/60000 (99%)]\tLoss: 0.090977\n",
      "\n",
      "Test set: Average loss: 0.0636, Accuracy: 9789/10000 (98%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.053495\n",
      "Train Epoch: 9 [640/60000 (1%)]\tLoss: 0.189524\n",
      "Train Epoch: 9 [1280/60000 (2%)]\tLoss: 0.035438\n",
      "Train Epoch: 9 [1920/60000 (3%)]\tLoss: 0.153907\n",
      "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 0.009333\n",
      "Train Epoch: 9 [3200/60000 (5%)]\tLoss: 0.023808\n",
      "Train Epoch: 9 [3840/60000 (6%)]\tLoss: 0.187125\n",
      "Train Epoch: 9 [4480/60000 (7%)]\tLoss: 0.081971\n",
      "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 0.106749\n",
      "Train Epoch: 9 [5760/60000 (10%)]\tLoss: 0.122909\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.070327\n",
      "Train Epoch: 9 [7040/60000 (12%)]\tLoss: 0.086137\n",
      "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 0.015361\n",
      "Train Epoch: 9 [8320/60000 (14%)]\tLoss: 0.012329\n",
      "Train Epoch: 9 [8960/60000 (15%)]\tLoss: 0.039586\n",
      "Train Epoch: 9 [9600/60000 (16%)]\tLoss: 0.104224\n",
      "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.186397\n",
      "Train Epoch: 9 [10880/60000 (18%)]\tLoss: 0.045817\n",
      "Train Epoch: 9 [11520/60000 (19%)]\tLoss: 0.089684\n",
      "Train Epoch: 9 [12160/60000 (20%)]\tLoss: 0.115218\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.070637\n",
      "Train Epoch: 9 [13440/60000 (22%)]\tLoss: 0.112401\n",
      "Train Epoch: 9 [14080/60000 (23%)]\tLoss: 0.025180\n",
      "Train Epoch: 9 [14720/60000 (25%)]\tLoss: 0.016302\n",
      "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 0.012196\n",
      "Train Epoch: 9 [16000/60000 (27%)]\tLoss: 0.022159\n",
      "Train Epoch: 9 [16640/60000 (28%)]\tLoss: 0.090722\n",
      "Train Epoch: 9 [17280/60000 (29%)]\tLoss: 0.011805\n",
      "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 0.125423\n",
      "Train Epoch: 9 [18560/60000 (31%)]\tLoss: 0.012354\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.257598\n",
      "Train Epoch: 9 [19840/60000 (33%)]\tLoss: 0.017833\n",
      "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.052478\n",
      "Train Epoch: 9 [21120/60000 (35%)]\tLoss: 0.115616\n",
      "Train Epoch: 9 [21760/60000 (36%)]\tLoss: 0.141686\n",
      "Train Epoch: 9 [22400/60000 (37%)]\tLoss: 0.051870\n",
      "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 0.088918\n",
      "Train Epoch: 9 [23680/60000 (39%)]\tLoss: 0.011833\n",
      "Train Epoch: 9 [24320/60000 (41%)]\tLoss: 0.066866\n",
      "Train Epoch: 9 [24960/60000 (42%)]\tLoss: 0.074221\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.172678\n",
      "Train Epoch: 9 [26240/60000 (44%)]\tLoss: 0.053303\n",
      "Train Epoch: 9 [26880/60000 (45%)]\tLoss: 0.007973\n",
      "Train Epoch: 9 [27520/60000 (46%)]\tLoss: 0.043488\n",
      "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 0.047958\n",
      "Train Epoch: 9 [28800/60000 (48%)]\tLoss: 0.021511\n",
      "Train Epoch: 9 [29440/60000 (49%)]\tLoss: 0.052963\n",
      "Train Epoch: 9 [30080/60000 (50%)]\tLoss: 0.024585\n",
      "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.039734\n",
      "Train Epoch: 9 [31360/60000 (52%)]\tLoss: 0.050949\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.018215\n",
      "Train Epoch: 9 [32640/60000 (54%)]\tLoss: 0.068441\n",
      "Train Epoch: 9 [33280/60000 (55%)]\tLoss: 0.008486\n",
      "Train Epoch: 9 [33920/60000 (57%)]\tLoss: 0.116299\n",
      "Train Epoch: 9 [34560/60000 (58%)]\tLoss: 0.031051\n",
      "Train Epoch: 9 [35200/60000 (59%)]\tLoss: 0.029304\n",
      "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 0.133005\n",
      "Train Epoch: 9 [36480/60000 (61%)]\tLoss: 0.005940\n",
      "Train Epoch: 9 [37120/60000 (62%)]\tLoss: 0.095408\n",
      "Train Epoch: 9 [37760/60000 (63%)]\tLoss: 0.006616\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.129223\n",
      "Train Epoch: 9 [39040/60000 (65%)]\tLoss: 0.101560\n",
      "Train Epoch: 9 [39680/60000 (66%)]\tLoss: 0.040996\n",
      "Train Epoch: 9 [40320/60000 (67%)]\tLoss: 0.076823\n",
      "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 0.090389\n",
      "Train Epoch: 9 [41600/60000 (69%)]\tLoss: 0.084193\n",
      "Train Epoch: 9 [42240/60000 (70%)]\tLoss: 0.046069\n",
      "Train Epoch: 9 [42880/60000 (71%)]\tLoss: 0.072552\n",
      "Train Epoch: 9 [43520/60000 (72%)]\tLoss: 0.080641\n",
      "Train Epoch: 9 [44160/60000 (74%)]\tLoss: 0.111463\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.014705\n",
      "Train Epoch: 9 [45440/60000 (76%)]\tLoss: 0.040592\n",
      "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 0.010469\n",
      "Train Epoch: 9 [46720/60000 (78%)]\tLoss: 0.117943\n",
      "Train Epoch: 9 [47360/60000 (79%)]\tLoss: 0.140616\n",
      "Train Epoch: 9 [48000/60000 (80%)]\tLoss: 0.040909\n",
      "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 0.140768\n",
      "Train Epoch: 9 [49280/60000 (82%)]\tLoss: 0.012366\n",
      "Train Epoch: 9 [49920/60000 (83%)]\tLoss: 0.044465\n",
      "Train Epoch: 9 [50560/60000 (84%)]\tLoss: 0.130533\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.010023\n",
      "Train Epoch: 9 [51840/60000 (86%)]\tLoss: 0.017382\n",
      "Train Epoch: 9 [52480/60000 (87%)]\tLoss: 0.058965\n",
      "Train Epoch: 9 [53120/60000 (88%)]\tLoss: 0.058878\n",
      "Train Epoch: 9 [53760/60000 (90%)]\tLoss: 0.057197\n",
      "Train Epoch: 9 [54400/60000 (91%)]\tLoss: 0.086096\n",
      "Train Epoch: 9 [55040/60000 (92%)]\tLoss: 0.033325\n",
      "Train Epoch: 9 [55680/60000 (93%)]\tLoss: 0.024783\n",
      "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 0.087654\n",
      "Train Epoch: 9 [56960/60000 (95%)]\tLoss: 0.120119\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.052258\n",
      "Train Epoch: 9 [58240/60000 (97%)]\tLoss: 0.024328\n",
      "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 0.044461\n",
      "Train Epoch: 9 [59520/60000 (99%)]\tLoss: 0.081538\n",
      "\n",
      "Test set: Average loss: 0.0543, Accuracy: 9818/10000 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
    "#*******************************************************************************\n",
    "#Author: Seungmin.Jeong\n",
    "#Purpose: Pytorch code for CNN accelerator using Verilog hdl\n",
    "#Revision History: 2023.03.03\n",
    "#*******************************************************************************\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "batch_size = 64\n",
    "train_dataset = datasets.MNIST(root='./data/',\n",
    "                                train=True,\n",
    "                                transform=transforms.ToTensor(),\n",
    "                                download=True)\n",
    "test_dataset = datasets.MNIST(root='./data/',\n",
    "                                train=False,\n",
    "                                transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=False)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.conv1_out_np = np.zeros((1, 4, 24, 24))\n",
    "        self.mp1_out_np = np.zeros((1, 4, 12, 12))\n",
    "        self.conv2_out_np = np.zeros((1, 12, 8, 8))\n",
    "        self.mp2_out_np = np.zeros((1, 12, 4, 4))\n",
    "        self.fc_in_np = np.zeros((1, 192))\n",
    "        self.fc_out_np = np.zeros((1, 10))\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 4, kernel_size=5, bias=False) \n",
    "        self.conv2 = nn.Conv2d(4, 12, kernel_size=5, bias=False)\n",
    "        self.mp = nn.MaxPool2d(2)\n",
    "        self.fc_1 = nn.Linear(192, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        in_size = x.size(0)\n",
    "        x = self.conv1(x)\n",
    "        self.conv1_out_np = x.detach().numpy()\n",
    "        x = F.relu(self.mp(x))\n",
    "        self.mp1_out_np = x.detach().numpy()\n",
    "        x = self.conv2(x)\n",
    "        self.conv2_out_np = x.detach().numpy()\n",
    "        x = F.relu(self.mp(x))\n",
    "        self.mp2_out_np = x.detach().numpy()\n",
    "        x = x.view(in_size, -1) \n",
    "        self.fc_in_np = x.detach().numpy()\n",
    "        x = self.fc_1(x)\n",
    "        self.fc_out_np = x.detach().numpy()\n",
    "        return F.log_softmax(x)\n",
    "     \n",
    "model = Net()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        \n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    \n",
    "    for data, target in test_loader:\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "        pred = output.data.max(1, keepdim=True)[1] \n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    \n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    \n",
    "for epoch in range(1, 10):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0   0   0   3   0   0   3   2   0   0   9   0   8\n",
      "    8   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   3   7   9   5   0   0   7   1\n",
      "    0   0   1   5   3   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   6   4   0   0   0   1   3   0   4   0\n",
      "    3   8   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   1   4   6   6   4   3   0   0\n",
      "    4   7   0   1  10   6   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   8   2   2   4   0   0   0   0   5   4\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   2   2   1   7  15   0   6\n",
      "   12   8  11   6   1   7   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   6   3   0   0   8   3   0   9   7\n",
      "    0   0   9   0   0   3   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   5   0   0  61 172 247 255 255 166\n",
      "   44  18  62  37   0   6   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0  19  62 227 251 255 243 252 255\n",
      "  160 185 252  98   0   5   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   9   0  54 250 242 227  60  76  56 113\n",
      "   96 235 253  61   0   4   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   5  42 234 255 198  51   0   0  20  20\n",
      "  177 255 143   0   2   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0 165 246 182  30   0  11   2   0  58\n",
      "  255 255  46   0  10   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0 170 255 228  54   0   0  15   0   0 192\n",
      "  255 180   8   3   0   4   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0 185 249 223  13   3   2   0  18 148 253\n",
      "  255  59   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0 205 255 234  11   0   0  42 131 250 227\n",
      "  242  10  12   3   7   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0 167 255 243 200 215 208 221 255 254 255\n",
      "  152   1   0   0   3   3   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   9 189 250 253 249 255 235  85 173 255\n",
      "   53   3   1   0   6   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0  60 116 112  72  10 141 235 219\n",
      "   28   0   3   0   3   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  23   0  15   4   0  19   0 251 255 145\n",
      "    9   0   7   2   4   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  40 255 229  57\n",
      "    3   0   2   0   2   5   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   8   0  11  10   0 144 254 159   1\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   2   8   5   0   0  50 238 255 108   0\n",
      "    0   0   0   1   3   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   5   0   2   8  14  95 251 245  56   9\n",
      "    0   0   9   0   2   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   5   0   0   0 113 255 211   3   8\n",
      "    0   1  11   0   0   3   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  11   5   0  13   0 204 255 112   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   4   0   0   1 172 255 119   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  16   7   0  11  84 179  68   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   1   0   0   0   0   0  37   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]]\n",
      "[[  0   0   0   0   0   0   0   0   0   4   0   0   3   0   0   0   0   0\n",
      "    0   1   0   3   4   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   3   4   0   0   0   2   6   0   2\n",
      "    6   5   0   0   0   1   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   8   2   0   1   7   5   0   0   0   4\n",
      "    1   0   0   0   0   7   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  10   0   0   7   0   0   0  20   1   6\n",
      "    0   0   3   5   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0  10   2   0   0   0   0   2\n",
      "    3   0   4   6   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   3  15   0   0   4  18   0   5   0\n",
      "    0   4   0   0   2   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  18   0   1  62 147 233 250 203 112  22\n",
      "    0   5   0   0   5   5   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  47 177 255 250 219 225 232 254  84\n",
      "    0  10   3   0   4   0   0   0   0   0]\n",
      " [  0   0   2   0   0   0   5   0  18 209 251 230  64  22   0  33 209 174\n",
      "    0  93  30   9  10   0   0   0   0   0]\n",
      " [  0   0   2   1   4   0   0   9 134 253 225   3   0   0  11   0  34  19\n",
      "   81 255 239  13   0   0   0   0   0   0]\n",
      " [  3   3   0   0   5   2   5  76 251 244  13  10   5   1   0  23  10  50\n",
      "  243 239 183   0  17   2   0   0   0   0]\n",
      " [  0   3   0   0   6   0   5 139 246 199   0   6   0  12   8   0   0   5\n",
      "  246 255 163   0   0   8   0   0   0   0]\n",
      " [  0   1   6   4   7   0   0 179 255  67  19   0   0   0   3   6   0 103\n",
      "  232 239  77  25   3   0   0   0   0   0]\n",
      " [  2   0   5   0   0   0  12 236 251  33   6   7   0  17  15   0  16 224\n",
      "  255 249  29   0   5   0   0   0   0   0]\n",
      " [ 11   0   1   0   0   7  20 255 255  59   0   0  15   0   4   0  64 240\n",
      "  255 147   2   0  10   4   0   0   0   0]\n",
      " [  4   0   7   8   0   6   0 221 255  99  13   0   0  18   6 136 255 255\n",
      "  255  35   8  13   0   1   0   0   0   0]\n",
      " [  0   1   2   0   0   1   9  16 242 255 185 105 153 156 223 238 232 255\n",
      "  228   9   7   4   0   0   0   0   0   0]\n",
      " [  0   1   2   0   0   0   4   9  78 199 255 236 240 210 166  75 246 255\n",
      "  159   3   0   0   1   0   0   0   0   0]\n",
      " [  0   1   2   0   0   0   0   1   1   7  49  65  42   3   0  29 253 255\n",
      "   71   1   0   0   5   0   0   0   0   0]\n",
      " [  0   0   1   0   0   0   0   0   0   0   4  15   3   0   0  75 247 255\n",
      "   14   4   1   6   1   0   0   0   0   0]\n",
      " [  0   0   1   1   0   0   0   0   0  10   3   0   0  10   8 140 248 255\n",
      "    0   4   7   8   0   7   0   0   0   0]\n",
      " [  0   0   0   1   1   1   1   1   5   4   0   5  14   0  24 236 255 205\n",
      "    3   1   0   2   0   9   0   0   0   0]\n",
      " [  0   0   0   0   0   1   1   0   0   3   0   0   0   0  68 255 253 100\n",
      "    1   4   0   0   3   1   0   0   0   0]\n",
      " [  1   0   0   0   0   0   0   0   4   0   0   4   3   0  85 246 241   9\n",
      "    0  11   0   0  14   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0  11   2   0   0 195 250 206   3\n",
      "    0   0   0   1   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  10   0   0   0   9  34 210 255 127   0\n",
      "    2   2   0   3   5   1   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0  10   9   5   0  11   1  33   0\n",
      "    3   2   0   0   7   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   7   2   0   0   0   6   2   3   0   0\n",
      "    2   3   0   0   3   0   0   0   0   0]]\n",
      "[[  0   0   0   0   0   0   0   0   0   0   1   6   2   0   0   1   0   0\n",
      "    9   0   7   0   4   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   7   0   0   0   0   0   0   0  12   5\n",
      "    7   0   0   4   0   2   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   1   2   2   4   7   6   2   0   3\n",
      "    0   9   2   0  20   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   4   7   0   0   0   0   0  13   3\n",
      "    1   0   0   0   5   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   5   8   7   3   0\n",
      "    9   0   0  23   0  10   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   1   0   0   4   8   2   0   0   0  32\n",
      "    0   3   0  13   0   9   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   7   2   3   2   0   0   9  38 136 188\n",
      "   52   3   0   0   7   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   1   1   1  41 144 242 250 255\n",
      "  225  43  78  55   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  10   0   1   0  38 240 254 254 254 255\n",
      "  246 243 251 202   1   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   4  15 151 254 245 245 245 252 247\n",
      "  255 237 255 224  11   1   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   5   3 170 244 253 242 121  66 200 255\n",
      "  255 249 241  42  20   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0 175 248 248 223  56 111 212 253 255\n",
      "  254 255 181   0   3   2   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0 139 252 252 221  71 243 255 248 253 255\n",
      "  246 221  59   9   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0 221 244 255 245 249 255 223 143 199 251\n",
      "  255 116   0   0   8   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0 201 255 250 238 255 127   8  48 249 237\n",
      "  216  15   0  27   1   5   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  16  86  96  36   0   0  11 177 248 255\n",
      "   29   0  10   0   0   2   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   5   0   7   6   0   4 106 252 254 155\n",
      "    4   0  11   0   3   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  11   0   0  14   7 215 251 224  19\n",
      "    0  22   0   0   0   2   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   3   0  21   7   3 190 247 255 101   0\n",
      "    3   0   0  19   0   4   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   5   1   0   0  43 246 255 169   0  24\n",
      "    5   0   6   8   0   3   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   2  12  32 237 241 247  19   0   4\n",
      "    0  13   3   0   9   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  12   0   6  88 236 255 112   5  19   0\n",
      "    0   8   0   3   6   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  13  60 229 243 255  46  13   3   0\n",
      "    9   0   0  23   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  12 185 249 255 224  33   0   0   6\n",
      "   10   0   0   3   0   8   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   7  90 255 255 224  40  11   1   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   1 132 226 126  44   0  10   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   5   2   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   7   9  16   0   0   0  11   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]]\n",
      "[[  0   0   0   0   0   0   0   0   0   1   6   4   0   0   5  10   0   0\n",
      "    1   0   2   5   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   4   0   0   0   0   0   0   0   8   0\n",
      "    0   0   1   0   4  13   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   3   9   9   4   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   1   2   3   3   0   0   0   0   0   8\n",
      "   12   2   2  13  13   2   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   5   0   0   0   0   0   4  13   3   4\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   1   0   0   5  11   4   0   0   0   0\n",
      "    0   7   9   0   0   9   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   4   0   0   2   0   0   0   6   0  11\n",
      "   13   1   0   0   0   3   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0  24  89 150 172 190\n",
      "  146  46   0   4  13   0   0   0   0   0]\n",
      " [  0   0   1   0   0   0   2   7   0   8   0  70 189 232 255 247 255 253\n",
      "  255 155   1   0   0   4   0   0   0   0]\n",
      " [  1   4   0   0   0   8   2   0  10  37 181 238 255 248 207 193 123 169\n",
      "  241 255  77   0  22   0   0   0   0   0]\n",
      " [  0   1   0   0   3   0   0   0 105 240 255 255 161  90   0  15  14   0\n",
      "  173 249 103   6   0   0   0   0   0   0]\n",
      " [  0   0   0  13   4   0  24 110 255 254 198  50   5   0   0   2   0   0\n",
      "  188 255 167  14   0  12   0   0   0   0]\n",
      " [  4   0   0   2   0   0  95 246 240 190  18   0  14  10   0   0  16  33\n",
      "  233 243 255 117   0   4   0   0   0   0]\n",
      " [  0   7   1   0   0  15 123 255 223   0  13   0   0   0  19  26  57 172\n",
      "  247 255 247  32  12   0   0   0   0   0]\n",
      " [  0   7   3   0   3  10 103 246 209  66  38  45  16  88 169 250 238 255\n",
      "  255 253 166  15   0   0   0   0   0   0]\n",
      " [  2   3   0   1   1   0  90 255 254 252 255 237 217 255 255 241 255 238\n",
      "  247 255  98   0   4   5   0   0   0   0]\n",
      " [  0   0   0   9   0   0  24  83 230 243 255 251 249 221 136  75  92 223\n",
      "  246 225  19   0   0   0   0   0   0   0]\n",
      " [  5   0   0   2   0   0   0  13  13  51  84  52  14   0   3   0  82 252\n",
      "  255  89   7   0   9   0   0   0   0   0]\n",
      " [  0   1   0   0   3   7   0   0   0   0  10   0   0   0   9   0 157 255\n",
      "  201   2   6   0   2   1   0   0   0   0]\n",
      " [  0   0   4   0   0   7   8   1   0   0   6   2   3   8   0  39 247 248\n",
      "   92   0   0   0   0   2   0   0   0   0]\n",
      " [  0   0   1   0   0   0   0   4   0   3   6   0   0   6   0 120 255 233\n",
      "   13   0   0  13   2   0   0   0   0   0]\n",
      " [  7   0   0   2   5   0   0   0   8   2   0   0   0   8   0 159 246 204\n",
      "    0   0  15   0   0   8   0   0   0   0]\n",
      " [  2   0   0   2   6   4   0   0   0   3  11   3   0   0  71 218 255 137\n",
      "    1   0  19   0   0   8   0   0   0   0]\n",
      " [  0   1   4   0   0   0   0   3   0   0   3   0   6   2 168 255 252  72\n",
      "    6   1   0   8  16   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  11   0   0  14   0  56 243 255 134   8\n",
      "    0   7   0   4   0   4   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   6   0  16   0   9 163 255 221  71   0\n",
      "    0   5   0   9   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  15   0   0   9  70 163  88  12   0\n",
      "    4   0   0   9   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   7   0   5   5   0   0   9   5   0   0\n",
      "    7   0   0   4   0   5   0   0   0   0]]\n",
      "[[  0   0   0   0   0   0   0   0   0   0   9   0   0  12   5   0   5   0\n",
      "    0   5   5   0   0   4   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  12   8   0   8   1   0   0   0   0   2\n",
      "    7   0   0   0   3   2   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  18   0   0   0  12   0   6  13   0\n",
      "    0   1   5   0   0   1   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  18   0   5  26   0   2   0   4   0   0\n",
      "    0   0   0   0   1   5   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   5   0   0   1  12   0   4   0  12\n",
      "   18   2   0   1   4   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  26   0   0   9   0   3   0   9   0\n",
      "    0   0   7   1   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   5   0 160 250 204 168 199 152  83  42\n",
      "    6   0   2   0   0   6   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   1  62 217  86  70 163 187 233 239 248\n",
      "  181  53   0   0   9   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0 171 198   0  17   9   0   3   0 111\n",
      "   76  23   3   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0 205 157   0   0   5   0  16   3  11\n",
      "    0   0   5   5   0  12   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  19 215 134   0  14   0   3   0   3   0\n",
      "   11   0   4   9   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  39 228 130   5   0   4   0   5  10   0\n",
      "   17   0   0   1   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0 177 239  18  19   0  13   0 123  68\n",
      "    0   0   5   0   0   8   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0 140 255  91   0  11   0   2 209 130\n",
      "    2   2   2   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  21  72 243 122   0   0   6  82 206  92\n",
      "    3   0   0   1  10   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   5 196 225  10   6   0 159 255  55\n",
      "    0   5   5   0   1   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   2   0 126 241 126   0  25 194 253  52\n",
      "    3   0   2   2   0   2   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   1  11   0 204 255 168 188 183 255  56\n",
      "    6   2   2   1   0   1   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   1   5 119 140  10 158 253  51\n",
      "    1   0   0   1   0   4   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   9   3   2   5   3   9   4  84 255  52\n",
      "    2   0   0   1   0   5   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   5   0   0  10   0   1  11 105 255  60\n",
      "    8   3   2   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  16  10   0   8   0   0 171 248  47\n",
      "    0   0   1   1   0   2   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   7   7   0   8   0 125 235  36\n",
      "    0   0   0   2   0   4   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  14   0   0   0   0   3 107 247  48\n",
      "    2   1   4   2   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   8   0   5   0   0 107 251  29\n",
      "    7   7   0   0   0   5   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   4   0   2   0   7  12  18 106 218  29\n",
      "    0   0   0  14   8   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   9   0   6   0   0   0   0  19  19   0\n",
      "    5  12   0   0   1   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   8   6   3   6   0   0   0   0\n",
      "    2   9   4   0   2   0   0   0   0   0]]\n",
      "[[  0   0   0   0   0   0   0   0   2   5   0   0   0   0   0   6   0   2\n",
      "    6   0   0   2   6   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   4   9   8   1   0   0   0\n",
      "    2  11  13   0   0   8   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   3   0   2   0   0   0   4   7   5   0\n",
      "    0   0   6   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   9  10   0   0   1   0   0  10\n",
      "   20   0   1   0   5   7   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   5   0   0   0   0   9  10   0   1   0\n",
      "    0   1  19   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   6   4   0   0   0   0   0  15   8\n",
      "    3   0   0   1   7   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   1  10   0   0  69 147 206 255 214 252\n",
      "  240 150   0   0   6   3   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   5   0  51 192 255 253 250 253 255\n",
      "  255 201  36   9   2   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   5   0  21 213 239 255 255 246 233 246\n",
      "  255 255 121  12   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   9  97 255 255 251 139  54  26 163\n",
      "  252 252 172   1   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   9   0 173 227 244 230  44   0  15 186\n",
      "  255 255 189   0   0   4   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0  39 255 254 222  94 125 181 255\n",
      "  247 255 134  15   0   8   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  11   1  12 181 248 255 255 252 251 255\n",
      "  247 248  57  16   0   2   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0  10  47 210 243 255 246 255\n",
      "  255 199  11   0   2   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   5   0   0   0  29  77 235 248 255 255\n",
      "  208  96   0   0   3   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  11   0   0   0 108 245 253 252 245\n",
      "   78   0   0   7   0   3   0   0   0   0]\n",
      " [  0  10   4   0   0   0   0   1   0  12   0  11  94 240 255 245 255 138\n",
      "   19   0   0   5   1   2   0   0   0   0]\n",
      " [  0   0   0   1   5   2   2   0   5   0   0  81 241 253 255 179  95  41\n",
      "    0   0   7   0   0   0   0   0   0   0]\n",
      " [  5   0   0   2   0   0   0   0   4   0  96 255 253 255 218   9   0   0\n",
      "    0   4   4   0   0   3   0   0   0   0]\n",
      " [  1   0   9   4   0   0   3   2   0 104 227 242 255 247 143   2   6   2\n",
      "    0   0   0   0   1   3   0   0   0   0]\n",
      " [  0   0  13   2   0  11   0   0  32 224 255 255 255 210  18   0   0   0\n",
      "    0   0   1   3   0   0   0   0   0   0]\n",
      " [  5   0   6   0   2   5   0   5 207 236 255 249 189  18  13   0   0   0\n",
      "    2   1   0   0   0   0   0   0   0   0]\n",
      " [ 10   0   2   0   1   0   0  98 244 255 253 239 107   0   0   8   4   4\n",
      "    3   0   0   0   2   8   0   0   0   0]\n",
      " [  1   0   4   0   4   0  50 227 255 255 244 142   0   0   8   0   3   0\n",
      "    0   0   3   1   0   0   0   0   0   0]\n",
      " [  0   0  10   5   3   0   0 212 246 255 167  37   0   0   0   2   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  8   5   0   0   0   6  20 177 255 228  47   0   0   0  17   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   4   2   2   6   0   0  21   0   1   0   1  13   0   2   9   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   4   0   0   6   2   0   0   3   0  12   0   0   4   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]]\n",
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   3   0  13\n",
      "    0   6   2   0   3   1   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   8   1   0   0   4   5   4   3   6   0\n",
      "    2   0   1   2   3   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   3   0   0   5   2   1   0   0   1   0\n",
      "   12   0   6   8   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   4   3   0   0   5   0  11   0\n",
      "    4   0   0   0   0  11   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   1   5   4   0   0   6  12   3   3   5\n",
      "    0  18   5   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   6   0   0   4   0   0   0   0  13\n",
      "    0   0   2   0  11   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   4   0   1  11   0   9  79 222 226\n",
      "  150   7   0   0   8   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   1   9   0   0   4   0  76 233 243 255\n",
      "  234  13  10   0  12   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   4   8   0   5  17 236 255 190 172\n",
      "  254   5   2   7   0   3   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   3   0  23  44 200 253 176   0  58\n",
      "  255 120   6   0   0   2   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  11   0   0  87 206 241 118  10  24  60\n",
      "  252 148   0   6   1   1   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  10  66 230 251 142   5  13 116 220\n",
      "  253 190   0   0   9   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  17 248 250 158  52 141 227 246 255\n",
      "  250  53   5   0   7   2   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  21  44 236 255 195 255 251 255 255 233\n",
      "  228   0  18   0   0   6   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  32 209 246 255 233 247 244 249 190\n",
      "   18   0   0   0  16   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0  39 101  93  54 243 255 147   0\n",
      "    7   0   4  15   0   3   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   2   0   6   0   0 164 247 152   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  15   0   0 103 245 253  37   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   9   0   0 168 255 206  38   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  16   0   0  46 224 255  73   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0  14 150 255 220   6   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   5   4 224 246 103   0  14   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   7   0  13 248 253  37   8   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   7 111 255 249  58 110  67   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   3   0  36 255 242 255 233  45   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   7   6 185 255 201  64   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   4   3   0   0   5   7   0  23   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0  26   4   0  12  13   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]]\n",
      "[[  0   0   0   0   0   0   0   0   3   0   0   0   0   0   0   0   0   4\n",
      "    1   0   0  13   0   8   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   1   1   0   1   3   0   4\n",
      "    0  16   2   0   0   1   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   1   0   0   0   0   1   1  18   3\n",
      "    0   4   0   2   4   1   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   2   2   0   0   0   3   3   0   0   0\n",
      "   13   6   0   9   0  12   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   3   4   1   0   0  10  11\n",
      "    7   0   4   0   0   1   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   1   6   0   0   0   9   0   0\n",
      "    0   0   8   0  32   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   3   0   0   4   0   0  39  95 255 225\n",
      "  230  70   0  31 109   9   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   2   0   0   2   0  24 114 209  78  18\n",
      "  167  11  29 222 233  20   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  12   0   3  15 167 149  59   4   0\n",
      "   59   6 205 206  51   6   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  12   0  20   0  89 201  29   0   0   0\n",
      "    0  56 213  91   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   3   0  19 116 132  18   0   7   0\n",
      "   56 242 235  44   4   6   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   6   0 134 195   0   1  11   0   3\n",
      "  154 241  78   0   7   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  10   0 105 227  13  14   6   0  25 155\n",
      "  237 145   6  13   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0 216  93  14   0   0  87 194 211\n",
      "  118   4   0   5   0  13   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  14  57 231  20  24  69 142 246 250  88\n",
      "    0   0  12   0   3   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  39 250 206 247 220 213 249 202   0\n",
      "    0   4   5   0   4   4   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   7   0 104 196 127  23 133 252 130  15\n",
      "    0  10   0   0   5   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   4  23   0   4  87 255 193  26   0\n",
      "    0  10   0   0   9   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  19   0   0  10  15 255 242  20   0   0\n",
      "    7   1   0   0   8   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  24   0   0 230 196  20   5  10   4\n",
      "    0   0   1   0   0   7   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   2   0   6 105 245  10   0  10   3   0\n",
      "    0   2  17   4   0   1   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   2   5  80 247 112  10   6   0   0   0\n",
      "    0   0   4   3   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0 100 216 170   5   0   3   0   1   7\n",
      "    6   0   0   0   3   3   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   7 203 221  16   5   0  10   0   2   0\n",
      "    0   5   1   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   3 208 125   0   0   0   7   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  19  80  17   0  20   0   0   5   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  10   3   0   2   2   0   4   1   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0  15   8   0   0   9   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]]\n",
      "[[  0   0   0   0   0   0   0   0   0   0   0   6   0   0   0   0   0   0\n",
      "    0   0   0   0   0   2   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   7  12   3   0   4  11   3   0   1\n",
      "    3   3   5   6   1   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   9   0   0   0   3   4   0   0   0   0\n",
      "    0   0   0   5   1   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   5   7   6   5   4   0   0   4   1   3\n",
      "    1   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   4   0   0   0  11   7   1   0   0\n",
      "    5   7   2   0   0   3   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   7  12   8  11  14   0   0   4   5   0\n",
      "    0   2   8   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   2   0  21 117 237 255 252 249 108  42\n",
      "    0   0   4   0   0   1   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  49 183 255 252 250 255 253 252 255 136\n",
      "   22   0   1   0   0   7   0   0   0   0]\n",
      " [  9   0   2   0   0   6   0   0 184 255 255 241 253 250 245 254 249 255\n",
      "  113   0   0  16   0   7   0   0   0   0]\n",
      " [  3   0   9   4   0   7   4 130 253 244 253 208 132  30  11 158 255 247\n",
      "  181  17   6   0   6   0   0   0   0   0]\n",
      " [  0   0   3   3   0   0   3 243 253 244 125  11   0   0   6   7 214 255\n",
      "  255 122   0   0  10  11   0   0   0   0]\n",
      " [  0   0   0   0   0   0  18 255 255 140   8   0   3   4   8   0  78 225\n",
      "  247 216  40   4   0   0   0   0   0   0]\n",
      " [  0   4   0   0  10   2  64 253 221  22  11   5   0  16   0  26 217 255\n",
      "  255 238 147   0   8   0   0   0   0   0]\n",
      " [  0   4   7   0   7   4  91 255 248  22   0  11   0   0  47 186 247 248\n",
      "  241 255 226   8  10   0   0   0   0   0]\n",
      " [  0   1   4   0   0   2  60 255 239 133   3   0   9  39 162 255 255 254\n",
      "  244 255 143   0   0   7   0   0   0   0]\n",
      " [  5   1   0   0   0   5  12 246 251 220  95  51 159 253 255 247 251 243\n",
      "  255 238  73   5   2   0   0   0   0   0]\n",
      " [  0  10   0   0   7   0  18 119 255 240 254 255 248 254 255 244 213 250\n",
      "  255 184   0  21   0   9   0   0   0   0]\n",
      " [  0   6   0   0  15   0   0  24 101 178 247 245 235 246 175  42  78 236\n",
      "  239  93   1   0  16   0   0   0   0   0]\n",
      " [  0   4   0   0   6   7   0   0   0   0  52 105  71   8   0   0  12 228\n",
      "  247  25  11   0  10   0   0   0   0   0]\n",
      " [  0   2   1   0   0   5  11   3   0   1   5   0   0   5  15   0  29 225\n",
      "  203   0   0   7   0   5   0   0   0   0]\n",
      " [  0   0   8  11   0   0   2   0   8   0   0   7   7   3   1   0  84 244\n",
      "  143   2   0   0   6   0   0   0   0   0]\n",
      " [  3   0   0   6   1   0   0   0   0   3   4   0   0   0   0   6 159 252\n",
      "  185  10   0   0  18   0   0   0   0   0]\n",
      " [  8   1   0   0   0   4   4   3  14   0   0   0  16  10   0   0 181 245\n",
      "  243  31   0   7   0   0   0   0   0   0]\n",
      " [  0   7   4   0   0   3   0   0   0   5   6   0   0   0   0   1 160 255\n",
      "  245  81  10   0   0   5   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 156 255\n",
      "  245  80  13   0   0   5   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 103 237\n",
      "  237  65   0   0   9   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   5   0   2  12   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   9   0\n",
      "    0   2   5   0   0   0   0   0   0   0]]\n",
      "[[  0   0   0   0   0   0   0   0   0  18   0   0   3   0   2   6   0   0\n",
      "    0   0   5   0   0  11   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   8   0   2  12   0  10   0\n",
      "    0   0   0  10   6   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   3  13   5   0   0   0   9   0   0\n",
      "    0   7   0   0  10   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  11   0   0   1   0   9   6   2   0   0\n",
      "    0   8   3   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  11   0   0  15   3   0   3   7  14\n",
      "    0   0   8   2   0   6   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  20   2   0   2   0   0   6   0   0\n",
      "    9   0   0   6   0   3   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  22 138 255 249 255 255 161  55   0\n",
      "   10   5   0   3   0   2   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  61 220 243 255 252 195 226 255 225  18\n",
      "    0   2   0  14   0   6   0   0   0   0]\n",
      " [  1   2   9   2   0  10   0  67 240 255 224  94  46  12  48 180 255  83\n",
      "    4   7   0  15   0   1   0   0   0   0]\n",
      " [  0   9   0   0  11   0  17 198 255 193  39   0   0   0   4  28 224 242\n",
      "   51   0  10   2   0   2   0   0   0   0]\n",
      " [  0   4   0   7   0   1 126 255 255  83   0   0   8  17   0   6  34 135\n",
      "   59   8   0   1   0   8   0   0   0   0]\n",
      " [  2   0   0  10   0   0 216 248 186   3  11   6   6   0   0   1   0   0\n",
      "    0  10   0  22   7   0   0   0   0   0]\n",
      " [  0   6   0   0  18   0 239 255 119   0   0   0   0   0  35   0   0  15\n",
      "    0  17   0   0   0   4   0   0   0   0]\n",
      " [  0   5   0   0   2   0 245 255 128   6   0   4   2   2   0   0   0   7\n",
      "    3   0   7   8   0   2   0   0   0   0]\n",
      " [  1   0  15   5   0   6 162 240 231  89  62   3  41 118 138 233 222 202\n",
      "  223 126  80   7   0   5   0   0   0   0]\n",
      " [  1   0   0   0  10   0   7 218 254 246 249 161 231 253 255 240 244 255\n",
      "  253 243 227  24  10   0   0   0   0   0]\n",
      " [  4   0   0   5   1   0  21  56 196 153 149 156 151 170 145  57  62  37\n",
      "   51 156 237  50   0   4   0   0   0   0]\n",
      " [  4   0   0   5   0   0   0  10   5   0  13  10   0   0  12   0   0   1\n",
      "   14 222 255 124   0   0   0   0   0   0]\n",
      " [  0   0   0   2   4   0   0   0   0   0   1   0   0   0   8   5   0   0\n",
      "    0 235 243 141   4   7   0   0   0   0]\n",
      " [  0   0   0   0   1   6   5   0   8   0   0   0   7   6   0   0  11   0\n",
      "   60 246 252  97   0   0   0   0   0   0]\n",
      " [  0   3   1   0   0   0   1   0   0   0   0   0   6   0   0   2   0   0\n",
      "  130 255 245  38   0   0   0   0   0   0]\n",
      " [  0   1   4   2   0   0   0   0   0   1   0   0   1   0   0   8   0   3\n",
      "  160 251 197   0   0  12   0   0   0   0]\n",
      " [  0   0   1   5   5   2   2   4   9   4   0   0   0   5   3   0   0   4\n",
      "  177 255 186   0   0   0   0   0   0   0]\n",
      " [  9   0   0   0   0   0   0   0   0   0   6   4   0   0   3   0   4   3\n",
      "  179 255 183   2   6   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  12\n",
      "  155 253 255  27   3   1   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   9   0\n",
      "   35 180 167  11   0  17   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  10\n",
      "    0  13   0   9   3   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  10\n",
      "    0   0   0   0   9   0   0   0   0   0]]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [28, 28]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21796\\1007408274.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[1;31m#100. * correct / len(test_loader.dataset)))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m \u001b[0msample_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21796\\1007408274.py\u001b[0m in \u001b[0;36msample_test\u001b[1;34m()\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m     \u001b[1;31m# model을 feed-forward 되어 나온 출력값\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m     \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m     \u001b[1;31m#print(np.shape(output))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\user\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21796\\3745148395.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[1;31m# CNN 층 출력이 최대 풀링 층을 지나 활성함수 ReLU를 지난다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1_out_np\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\user\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\user\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    462\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 463\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    464\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\user\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    457\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m--> 459\u001b[1;33m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[0;32m    460\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[0;32m    461\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [28, 28]"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "from fxpmath import Fxp\n",
    "\n",
    "\n",
    "def sample_test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    target = Variable(torch.tensor([0]))\n",
    "    img0 = Image.open(\"C:\\\\Users\\\\user\\\\Desktop\\\\MNIST_test_image\\\\9\\\\img9 (1).jpg\", \"r\")\n",
    "    np_img0 = np.array(img0)\n",
    "    np_img_re0 = np.reshape(np_img0,(28,28))\n",
    "    print(np_img_re0)\n",
    "    np.savetxt('HW_test_num9_1.mem', np_img_re0, fmt='%1.2X',delimiter = \" \")\n",
    "    np.savetxt('SW_test_num9_1.txt', np_img_re0, fmt='%d',delimiter = \" \")\n",
    "    img0 = Image.open(\"C:\\\\Users\\\\user\\\\Desktop\\\\MNIST_test_image\\\\9\\\\img9 (2).jpg\", \"r\")\n",
    "    np_img0 = np.array(img0)\n",
    "    np_img_re0 = np.reshape(np_img0,(28,28))\n",
    "    print(np_img_re0)\n",
    "    np.savetxt('HW_test_num9_2.mem', np_img_re0, fmt='%1.2X',delimiter = \" \")\n",
    "    np.savetxt('SW_test_num9_2.txt', np_img_re0, fmt='%d',delimiter = \" \")\n",
    "    img0 = Image.open(\"C:\\\\Users\\\\user\\\\Desktop\\\\MNIST_test_image\\\\9\\\\img9 (3).jpg\", \"r\")\n",
    "    np_img0 = np.array(img0)\n",
    "    np_img_re0 = np.reshape(np_img0,(28,28))\n",
    "    print(np_img_re0)\n",
    "    np.savetxt('HW_test_num9_3.mem', np_img_re0, fmt='%1.2X',delimiter = \" \")\n",
    "    np.savetxt('SW_test_num9_3.txt', np_img_re0, fmt='%d',delimiter = \" \")\n",
    "    img0 = Image.open(\"C:\\\\Users\\\\user\\\\Desktop\\\\MNIST_test_image\\\\9\\\\img9 (4).jpg\", \"r\")\n",
    "    np_img0 = np.array(img0)\n",
    "    np_img_re0 = np.reshape(np_img0,(28,28))\n",
    "    print(np_img_re0)\n",
    "    np.savetxt('HW_test_num9_4.mem', np_img_re0, fmt='%1.2X',delimiter = \" \")\n",
    "    np.savetxt('SW_test_num9_4.txt', np_img_re0, fmt='%d',delimiter = \" \")\n",
    "    img0 = Image.open(\"C:\\\\Users\\\\user\\\\Desktop\\\\MNIST_test_image\\\\9\\\\img9 (5).jpg\", \"r\")\n",
    "    np_img0 = np.array(img0)\n",
    "    np_img_re0 = np.reshape(np_img0,(28,28))\n",
    "    print(np_img_re0)\n",
    "    np.savetxt('HW_test_num9_5.mem', np_img_re0, fmt='%1.2X',delimiter = \" \")\n",
    "    np.savetxt('SW_test_num9_5.txt', np_img_re0, fmt='%d',delimiter = \" \")\n",
    "    img0 = Image.open(\"C:\\\\Users\\\\user\\\\Desktop\\\\MNIST_test_image\\\\9\\\\img9 (6).jpg\", \"r\")\n",
    "    np_img0 = np.array(img0)\n",
    "    np_img_re0 = np.reshape(np_img0,(28,28))\n",
    "    print(np_img_re0)\n",
    "    np.savetxt('HW_test_num9_6.mem', np_img_re0, fmt='%1.2X',delimiter = \" \")\n",
    "    np.savetxt('SW_test_num9_6.txt', np_img_re0, fmt='%d',delimiter = \" \")\n",
    "    img0 = Image.open(\"C:\\\\Users\\\\user\\\\Desktop\\\\MNIST_test_image\\\\9\\\\img9 (7).jpg\", \"r\")\n",
    "    np_img0 = np.array(img0)\n",
    "    np_img_re0 = np.reshape(np_img0,(28,28))\n",
    "    print(np_img_re0)\n",
    "    np.savetxt('HW_test_num9_7.mem', np_img_re0, fmt='%1.2X',delimiter = \" \")\n",
    "    np.savetxt('SW_test_num9_7.txt', np_img_re0, fmt='%d',delimiter = \" \")\n",
    "    img0 = Image.open(\"C:\\\\Users\\\\user\\\\Desktop\\\\MNIST_test_image\\\\9\\\\img9 (8).jpg\", \"r\")\n",
    "    np_img0 = np.array(img0)\n",
    "    np_img_re0 = np.reshape(np_img0,(28,28))\n",
    "    print(np_img_re0)\n",
    "    np.savetxt('HW_test_num9_8.mem', np_img_re0, fmt='%1.2X',delimiter = \" \")\n",
    "    np.savetxt('SW_test_num9_8.txt', np_img_re0, fmt='%d',delimiter = \" \")\n",
    "    img0 = Image.open(\"C:\\\\Users\\\\user\\\\Desktop\\\\MNIST_test_image\\\\9\\\\img9 (9).jpg\", \"r\")\n",
    "    np_img0 = np.array(img0)\n",
    "    np_img_re0 = np.reshape(np_img0,(28,28))\n",
    "    print(np_img_re0)\n",
    "    np.savetxt('HW_test_num9_9.mem', np_img_re0, fmt='%1.2X',delimiter = \" \")\n",
    "    np.savetxt('SW_test_num9_9.txt', np_img_re0, fmt='%d',delimiter = \" \")\n",
    "    img0 = Image.open(\"C:\\\\Users\\\\user\\\\Desktop\\\\MNIST_test_image\\\\9\\\\img9 (10).jpg\", \"r\")\n",
    "    np_img0 = np.array(img0)\n",
    "    np_img_re0 = np.reshape(np_img0,(28,28))\n",
    "    print(np_img_re0)\n",
    "    np.savetxt('HW_test_num9_10.mem', np_img_re0, fmt='%1.2X',delimiter = \" \")\n",
    "    np.savetxt('SW_test_num9_10.txt', np_img_re0, fmt='%d',delimiter = \" \")\n",
    "    data = Variable(torch.tensor((np_img_re0 / 255), dtype = torch.float32))\n",
    "    \n",
    "    output = model(data)\n",
    "    test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "    pred = output.data.max(1, keepdim=True)[1]\n",
    "    correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "    print(correct)\n",
    "    \n",
    "sample_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signed\n",
      "[[ 21  47  -9   0  -5]\n",
      " [ 91  38  26 -40 -53]\n",
      " [127  84  19 -14 -47]\n",
      " [127 127  71  49  -4]\n",
      " [ 61  90  96  54  35]\n",
      " [-21   7 -12  -5   5]\n",
      " [-23 -24  19   8   7]\n",
      " [ -1   2  -9  25  -7]\n",
      " [ -3 -27  30  -1  15]\n",
      " [-21 -14  37  -4 -13]\n",
      " [-14 -50 -45 -44 -33]\n",
      " [-15 -30 -56 -64 -17]\n",
      " [ 28 -17 -19   4  54]\n",
      " [ 48  65  63  79  44]\n",
      " [ 60  69  65  88  32]\n",
      " [-10  -5   6  -6 -12]\n",
      " [ 14  -4   6 -25   9]\n",
      " [ -8 -12 -25   4   8]\n",
      " [ -7 -15 -24   4  -3]\n",
      " [  0 -15  -1 -16 -21]]\n",
      "tensor([[ 21,  47,  -9,   0,  -5],\n",
      "        [ 91,  38,  26, -40, -53],\n",
      "        [127,  84,  19, -14, -47],\n",
      "        [127, 127,  71,  49,  -4],\n",
      "        [ 61,  90,  96,  54,  35]], dtype=torch.int32)\n",
      "tensor([[-21,   7, -12,  -5,   5],\n",
      "        [-23, -24,  19,   8,   7],\n",
      "        [ -1,   2,  -9,  25,  -7],\n",
      "        [ -3, -27,  30,  -1,  15],\n",
      "        [-21, -14,  37,  -4, -13]], dtype=torch.int32)\n",
      "tensor([[-14, -50, -45, -44, -33],\n",
      "        [-15, -30, -56, -64, -17],\n",
      "        [ 28, -17, -19,   4,  54],\n",
      "        [ 48,  65,  63,  79,  44],\n",
      "        [ 60,  69,  65,  88,  32]], dtype=torch.int32)\n",
      "tensor([[-10,  -5,   6,  -6, -12],\n",
      "        [ 14,  -4,   6, -25,   9],\n",
      "        [ -8, -12, -25,   4,   8],\n",
      "        [ -7, -15, -24,   4,  -3],\n",
      "        [  0, -15,  -1, -16, -21]], dtype=torch.int32)\n",
      "Unsigned\n",
      "[[ 21  47 247   0 251]\n",
      " [ 91  38  26 216 203]\n",
      " [127  84  19 242 209]\n",
      " [127 127  71  49 252]\n",
      " [ 61  90  96  54  35]\n",
      " [235   7 244 251   5]\n",
      " [233 232  19   8   7]\n",
      " [255   2 247  25 249]\n",
      " [253 229  30 255  15]\n",
      " [235 242  37 252 243]\n",
      " [242 206 211 212 223]\n",
      " [241 226 200 192 239]\n",
      " [ 28 239 237   4  54]\n",
      " [ 48  65  63  79  44]\n",
      " [ 60  69  65  88  32]\n",
      " [246 251   6 250 244]\n",
      " [ 14 252   6 231   9]\n",
      " [248 244 231   4   8]\n",
      " [249 241 232   4 253]\n",
      " [  0 241 255 240 235]]\n",
      "tensor([[ 21,  47, 247,   0, 251],\n",
      "        [ 91,  38,  26, 216, 203],\n",
      "        [127,  84,  19, 242, 209],\n",
      "        [127, 127,  71,  49, 252],\n",
      "        [ 61,  90,  96,  54,  35]], dtype=torch.int32)\n",
      "tensor([[235,   7, 244, 251,   5],\n",
      "        [233, 232,  19,   8,   7],\n",
      "        [255,   2, 247,  25, 249],\n",
      "        [253, 229,  30, 255,  15],\n",
      "        [235, 242,  37, 252, 243]], dtype=torch.int32)\n",
      "tensor([[242, 206, 211, 212, 223],\n",
      "        [241, 226, 200, 192, 239],\n",
      "        [ 28, 239, 237,   4,  54],\n",
      "        [ 48,  65,  63,  79,  44],\n",
      "        [ 60,  69,  65,  88,  32]], dtype=torch.int32)\n",
      "tensor([[246, 251,   6, 250, 244],\n",
      "        [ 14, 252,   6, 231,   9],\n",
      "        [248, 244, 231,   4,   8],\n",
      "        [249, 241, 232,   4, 253],\n",
      "        [  0, 241, 255, 240, 235]], dtype=torch.int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_26236\\1049555460.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  int_conv1_weight_1 =  torch.tensor((model.conv1.weight.data[0][0] * 128), dtype = torch.int32)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_26236\\1049555460.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  int_conv1_weight_2 =  torch.tensor((model.conv1.weight.data[1][0] * 128), dtype = torch.int32)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_26236\\1049555460.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  int_conv1_weight_3 =  torch.tensor((model.conv1.weight.data[2][0] * 128), dtype = torch.int32)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_26236\\1049555460.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  int_conv1_weight_4 =  torch.tensor((model.conv1.weight.data[3][0] * 128), dtype = torch.int32)\n"
     ]
    }
   ],
   "source": [
    "############## Conv1 ############\n",
    "import numpy as np\n",
    "int_conv1_weight_1 =  torch.tensor((model.conv1.weight.data[0][0] * 128), dtype = torch.int32)\n",
    "int_conv1_weight_2 =  torch.tensor((model.conv1.weight.data[1][0] * 128), dtype = torch.int32)\n",
    "int_conv1_weight_3 =  torch.tensor((model.conv1.weight.data[2][0] * 128), dtype = torch.int32)\n",
    "int_conv1_weight_4 =  torch.tensor((model.conv1.weight.data[3][0] * 128), dtype = torch.int32)\n",
    "#It is to prevent overflow of weights. If the weight value exceeds 127 (011111111), MSB becomes 1 when it is converted into a Binary of 8bit.\n",
    "#However, in HW, this means negative. Therefore, the number exceeding 127 is fixed to 127.\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        if int_conv1_weight_1[i][j] > 127:\n",
    "            int_conv1_weight_1[i][j] = 127\n",
    "        if int_conv1_weight_2[i][j] > 127:\n",
    "            int_conv1_weight_2[i][j] = 127\n",
    "        if int_conv1_weight_3[i][j] > 127:\n",
    "            int_conv1_weight_3[i][j] = 127\n",
    "        if int_conv1_weight_4[i][j] > 127:\n",
    "            int_conv1_weight_4[i][j] = 127\n",
    "int_conv1_weight_stack = np.vstack((int_conv1_weight_1,int_conv1_weight_2,int_conv1_weight_3,int_conv1_weight_4))\n",
    "print(\"Signed\")\n",
    "print(int_conv1_weight_stack)\n",
    "print(int_conv1_weight_1)\n",
    "print(int_conv1_weight_2)\n",
    "print(int_conv1_weight_3)\n",
    "print(int_conv1_weight_4)\n",
    "np.savetxt('SW_conv1_weight.txt', int_conv1_weight_stack, fmt='%d',delimiter = \" \")\n",
    "\n",
    "# If the value of the weight is negative, then +256 based on 8 bits and extract it as the Hex value, HW accepts it as the correct negative value.\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        if int_conv1_weight_1[i][j] < 0:\n",
    "            int_conv1_weight_1[i][j] += 256\n",
    "        if int_conv1_weight_2[i][j] < 0:\n",
    "            int_conv1_weight_2[i][j] += 256\n",
    "        if int_conv1_weight_3[i][j] < 0:\n",
    "            int_conv1_weight_3[i][j] += 256\n",
    "        if int_conv1_weight_4[i][j] < 0:\n",
    "            int_conv1_weight_4[i][j] += 256\n",
    "\n",
    "int_conv1_weight_stack = np.vstack((int_conv1_weight_1,int_conv1_weight_2,int_conv1_weight_3,int_conv1_weight_4))\n",
    "print (\"Unsigned\")\n",
    "print(int_conv1_weight_stack)\n",
    "print(int_conv1_weight_1)\n",
    "print(int_conv1_weight_2)\n",
    "print(int_conv1_weight_3)\n",
    "print(int_conv1_weight_4)\n",
    "np.savetxt('HW_conv1_weight.mem', int_conv1_weight_stack, fmt='%1.2X',delimiter = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 4, 5, 5])\n",
      "Signed\n",
      "[[-25 -11   5  18  20]\n",
      " [-12  -9 -12  -3  24]\n",
      " [ 23  -8 -47 -58 -32]\n",
      " ...\n",
      " [-12  -9  -5  -4   6]\n",
      " [-11  -1  -9   0   9]\n",
      " [ -6   5   7  -3   3]] \n",
      "\n",
      "Unsigned\n",
      "[[231 245   5  18  20]\n",
      " [244 247 244 253  24]\n",
      " [ 23 248 209 198 224]\n",
      " [  9  23  20 243 243]\n",
      " [254   0  30  33 255]\n",
      " [230 216 218 238 228]\n",
      " [ 19 251 229 224 214]\n",
      " [ 13  16  26  27  11]\n",
      " [255   0  11  13  15]\n",
      " [  0 254 246 247   6]\n",
      " [245 236   7  14  11]\n",
      " [218 248  25   6 236]\n",
      " [193  26  46 244 223]\n",
      " [233  44  50 242 230]\n",
      " [ 19  39  26 254 254]\n",
      " [249 226 217 229   4]\n",
      " [228 237  10  16  21]\n",
      " [235   0   9  16 235]\n",
      " [  5  17  23 251 252]\n",
      " [ 18  33  14  22   8]\n",
      " [  2 237 224   0  38]\n",
      " [ 15 255 223 238  32]\n",
      " [ 31 254 221 238  38]\n",
      " [ 23 251 213   0  33]\n",
      " [  7 233 245  36  32]\n",
      " [  1 251 247 237 221]\n",
      " [  3  13 243 241 242]\n",
      " [251  19 246 235 254]\n",
      " [  1  18  14   0  10]\n",
      " [ 20  33  38  38  26]\n",
      " [ 11  39  43  23  14]\n",
      " [247 235 240 250 255]\n",
      " [  2 225 224 221 228]\n",
      " [254 253 236 254   3]\n",
      " [251 255 255   4   0]\n",
      " [255 255 237 224 240]\n",
      " [  8  23   2 232 240]\n",
      " [  4   9  21 231 229]\n",
      " [253  21  10 243 252]\n",
      " [253  11  13   5 245]\n",
      " [  9  27 252 201 215]\n",
      " [239  22  52  27 205]\n",
      " [224 232  36  47 248]\n",
      " [227 234  13  41   2]\n",
      " [  3 244 248  12 252]\n",
      " [231 254 245   0  10]\n",
      " [241  37   0 245  24]\n",
      " [246  34 239 243  23]\n",
      " [ 15  27 242   3  23]\n",
      " [  3  12   1 255   6]\n",
      " [ 10   4   7 238 241]\n",
      " [254  20  39  24 254]\n",
      " [227 234  14  39  18]\n",
      " [232 229 221 255   9]\n",
      " [247 234 232 235 234]\n",
      " [252 241   9  33  15]\n",
      " [ 20 229 218 246 226]\n",
      " [ 36   5 221 233 239]\n",
      " [ 24   7  12   3   0]\n",
      " [  5  13   6 255  17]] \n",
      "\n",
      "[[  4  11   3   0 249]\n",
      " [254 255 253 254 251]\n",
      " [  4 245 253   0 252]\n",
      " [250 242   2   2 249]\n",
      " [246   5   5   5 255]\n",
      " [  4   1 248 243  11]\n",
      " [243 249 244 249 253]\n",
      " [243  10   5   8 244]\n",
      " [253 251 250 247   4]\n",
      " [244 255  10 249 255]\n",
      " [246 250  16 252 247]\n",
      " [  2  15  11   3   5]\n",
      " [  1   6   6 254   2]\n",
      " [246 250   1 252 253]\n",
      " [  0  12   0   9   5]\n",
      " [  5 242   2 249   1]\n",
      " [250 251 252 245   0]\n",
      " [ 10   4   5   5 247]\n",
      " [ 10 255  10   4   4]\n",
      " [  4 253 251 255 247]\n",
      " [254   8   6   8 250]\n",
      " [  2  11   0   6   1]\n",
      " [ 12   5   1 250  13]\n",
      " [  8   9   9  12 247]\n",
      " [  6 250   7   9 255]\n",
      " [  6  12  11   3   0]\n",
      " [252   0 247 254  11]\n",
      " [ 14   3   7   9   9]\n",
      " [  3 253   4   0  10]\n",
      " [249 246 245 245   9]\n",
      " [  6 249 253 243 248]\n",
      " [253 254 249   9  10]\n",
      " [  7   0   8   5 255]\n",
      " [ 11 250 254   1 253]\n",
      " [  6 246   3   0   7]\n",
      " [  2   7   0   0 248]\n",
      " [244   2   1 248   6]\n",
      " [  0 246  11 245 253]\n",
      " [251 252   5   2 248]\n",
      " [253 248 255   3   6]\n",
      " [  5   8 248   4   2]\n",
      " [247 242   2   9 255]\n",
      " [254 253   6 250 249]\n",
      " [  5   3   9   3   0]\n",
      " [246   7 254   5  12]\n",
      " [  7  14   2  12 252]\n",
      " [ 10  14 253 251   9]\n",
      " [253 246   8   6 246]\n",
      " [  7 253   2 254   5]\n",
      " [  0   9   7 244  11]\n",
      " [243   0   0 252   8]\n",
      " [  7   2   5 253 247]\n",
      " [245 255 245   6 253]\n",
      " [ 11 244 253   5   1]\n",
      " [  1 252   7 247 246]\n",
      " [  3 252   6  13 251]\n",
      " [246   3   2 250 252]\n",
      " [  8   3   8 255   4]\n",
      " [245 245 248   0   3]\n",
      " [  3 255 245 246 250]] \n",
      "\n",
      "[[248 239   8  14  13]\n",
      " [  6 247 241   4   2]\n",
      " [ 18 246 237 237   1]\n",
      " [  6  24  32  26  21]\n",
      " [  1  30  52  22   4]\n",
      " [246 229 235 228 243]\n",
      " [  2   3  12 247 239]\n",
      " [ 27  24  16  22  15]\n",
      " [  6   0  12   0  25]\n",
      " [  9   5 252 237 247]\n",
      " [  1   4  11  11 253]\n",
      " [  1   8   3 228 239]\n",
      " [252 243 209 231 247]\n",
      " [244 198 240   2 243]\n",
      " [217 227 250   4 248]\n",
      " [247 245   4   6   0]\n",
      " [250  12  21   6 245]\n",
      " [ 20  28 243 230   0]\n",
      " [252 243 232   8  16]\n",
      " [241 241 253  12   6]\n",
      " [241   1 234 243   8]\n",
      " [ 12 255 229 248 255]\n",
      " [255 236   0   0 250]\n",
      " [235 241   0 248 240]\n",
      " [  5  11  28 255 245]\n",
      " [245 251 249 247 253]\n",
      " [  0   5 241 247  12]\n",
      " [240   1   3   1 245]\n",
      " [246 254  11   0 236]\n",
      " [ 10   5  12 251 253]\n",
      " [ 15  43  66  53  32]\n",
      " [249 252 250 242 251]\n",
      " [253 247 242 253 237]\n",
      " [  0  25  10   6 248]\n",
      " [ 18   7  14 249   0]\n",
      " [251 245 249 248 245]\n",
      " [247   0 253   2 239]\n",
      " [255 251   7 237 245]\n",
      " [ 17   0 244 255   1]\n",
      " [  5 240 240   0   3]\n",
      " [ 12   6  13 235 225]\n",
      " [240  17   7   1 234]\n",
      " [  3 228 251   8 241]\n",
      " [ 18 245 247 235 244]\n",
      " [  8  21 233 239   3]\n",
      " [ 10 249 245   1 254]\n",
      " [246 245 243 240 253]\n",
      " [242 234   7 241  14]\n",
      " [234   0   0 239  10]\n",
      " [244 250 255 245 253]\n",
      " [  1 252  20  18  22]\n",
      " [247  20  33  14  10]\n",
      " [237 239  12  19   9]\n",
      " [241 231 232 254  15]\n",
      " [ 14   5   9   5   4]\n",
      " [254 253 255   5 240]\n",
      " [221 228 234 241 247]\n",
      " [252  13   6 251   8]\n",
      " [248 255   3  11   9]\n",
      " [241 247 244  16  13]] \n",
      "\n",
      "[[  7  10  11   3  10]\n",
      " [  2  12 248 249 254]\n",
      " [  1 254   4  10 248]\n",
      " [  0  11   3 244   2]\n",
      " [250 253  11 248 251]\n",
      " [249   6   7   0  11]\n",
      " [  9 244   0   5  11]\n",
      " [253 244   0   3 249]\n",
      " [  0 245 250 254   7]\n",
      " [245 246   6  10 246]\n",
      " [  0  10 246   1 248]\n",
      " [252   5 249 253   1]\n",
      " [243   2  11   7   2]\n",
      " [  8   5   8 247   0]\n",
      " [  3 247   6   5 248]\n",
      " [253 245  12   4   9]\n",
      " [  1 249   0 255 246]\n",
      " [254 250 244 247 252]\n",
      " [ 10   1 247 250  10]\n",
      " [  7  11 249   6 255]\n",
      " [252 252   8  11   0]\n",
      " [255 251   9 248   9]\n",
      " [247   3   8 255 253]\n",
      " [ 12   9   0 246  10]\n",
      " [  0   9 250 250 246]\n",
      " [ 10 252 244   8 247]\n",
      " [250 254   5   5 246]\n",
      " [255 246 255 247 246]\n",
      " [249   8 248 249 249]\n",
      " [  4 249 253 245 245]\n",
      " [  7   2 244   6   0]\n",
      " [  2 247 250 245 250]\n",
      " [253   8 253   3 252]\n",
      " [  1   0   2 248   2]\n",
      " [255   3 252 250 252]\n",
      " [254   8   1 249 250]\n",
      " [  7   2  12 254   8]\n",
      " [254  10   7 253   6]\n",
      " [ 10 252 245  11 254]\n",
      " [  2 250 250   2   0]\n",
      " [ 12   1   8 251 246]\n",
      " [254 254   0 247   3]\n",
      " [246 249 247 244   3]\n",
      " [  9   8   5 253 255]\n",
      " [  6 255   8 252   5]\n",
      " [250   6   7   2 244]\n",
      " [  7  10 253 248   9]\n",
      " [  5 255 251   8 250]\n",
      " [  2   8   5   4 249]\n",
      " [248 244 247 246   6]\n",
      " [  8 244 255   0 249]\n",
      " [  2   8 252 252 244]\n",
      " [  4 255 251   9 246]\n",
      " [253  11 251   9 248]\n",
      " [254   6   1 244   2]\n",
      " [ 11 245 254   5  10]\n",
      " [  5 253 252   6 250]\n",
      " [244 247 251 252   6]\n",
      " [245 255 247   0   9]\n",
      " [250   5   7 253   3]] \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_26236\\219235275.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  int_conv2_weight_11 =  torch.tensor((model.conv2.weight.data[0][0]* 128), dtype = torch.int32)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_26236\\219235275.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  int_conv2_weight_12 =  torch.tensor((model.conv2.weight.data[0][1]* 128), dtype = torch.int32)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_26236\\219235275.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  int_conv2_weight_13 =  torch.tensor((model.conv2.weight.data[0][2]* 128), dtype = torch.int32)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_26236\\219235275.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  int_conv2_weight_14 =  torch.tensor((model.conv2.weight.data[0][3]* 128), dtype = torch.int32)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_26236\\219235275.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  int_conv2_weight_21 =  torch.tensor((model.conv2.weight.data[1][0] * 128), dtype = torch.int32)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_26236\\219235275.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  int_conv2_weight_22 =  torch.tensor((model.conv2.weight.data[1][1] * 128), dtype = torch.int32)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_26236\\219235275.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  int_conv2_weight_23 =  torch.tensor((model.conv2.weight.data[1][2] * 128), dtype = torch.int32)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_26236\\219235275.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  int_conv2_weight_24 =  torch.tensor((model.conv2.weight.data[1][3] * 128), dtype = torch.int32)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_26236\\219235275.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  int_conv2_weight_31 =  torch.tensor((model.conv2.weight.data[2][0] * 128), dtype = torch.int32)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_26236\\219235275.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  int_conv2_weight_32 =  torch.tensor((model.conv2.weight.data[2][1] * 128), dtype = torch.int32)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_26236\\219235275.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  int_conv2_weight_33 =  torch.tensor((model.conv2.weight.data[2][2] * 128), dtype = torch.int32)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_26236\\219235275.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  int_conv2_weight_34 =  torch.tensor((model.conv2.weight.data[2][3] * 128), dtype = torch.int32)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_26236\\219235275.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  int_conv2_weight_41 =  torch.tensor((model.conv2.weight.data[3][0] * 128), dtype = torch.int32)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_26236\\219235275.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  int_conv2_weight_42 =  torch.tensor((model.conv2.weight.data[3][1] * 128), dtype = torch.int32)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_26236\\219235275.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  int_conv2_weight_43 =  torch.tensor((model.conv2.weight.data[3][2] * 128), dtype = torch.int32)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_26236\\219235275.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  int_conv2_weight_44 =  torch.tensor((model.conv2.weight.data[3][3] * 128), dtype = torch.int32)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_26236\\219235275.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  int_conv2_weight_51 =  torch.tensor((model.conv2.weight.data[4][0] * 128), dtype = torch.int32)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_26236\\219235275.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  int_conv2_weight_52 =  torch.tensor((model.conv2.weight.data[4][1] * 128), dtype = torch.int32)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_26236\\219235275.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  int_conv2_weight_53 =  torch.tensor((model.conv2.weight.data[4][2] * 128), dtype = torch.int32)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_26236\\219235275.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  int_conv2_weight_54 =  torch.tensor((model.conv2.weight.data[4][3] * 128), dtype = torch.int32)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_26236\\219235275.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  int_conv2_weight_61 =  torch.tensor((model.conv2.weight.data[5][0] * 128), dtype = torch.int32)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_26236\\219235275.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  int_conv2_weight_62 =  torch.tensor((model.conv2.weight.data[5][1] * 128), dtype = torch.int32)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_26236\\219235275.py:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  int_conv2_weight_63 =  torch.tensor((model.conv2.weight.data[5][2] * 128), dtype = torch.int32)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_26236\\219235275.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  int_conv2_weight_64 =  torch.tensor((model.conv2.weight.data[5][3] * 128), dtype = torch.int32)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_26236\\219235275.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  int_conv2_weight_71 =  torch.tensor((model.conv2.weight.data[6][0] * 128), dtype = torch.int32)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_26236\\219235275.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  int_conv2_weight_72 =  torch.tensor((model.conv2.weight.data[6][1] * 128), dtype = torch.int32)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_26236\\219235275.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  int_conv2_weight_73 =  torch.tensor((model.conv2.weight.data[6][2] * 128), dtype = torch.int32)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_26236\\219235275.py:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  int_conv2_weight_74 =  torch.tensor((model.conv2.weight.data[6][3] * 128), dtype = torch.int32)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_26236\\219235275.py:40: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  int_conv2_weight_81 =  torch.tensor((model.conv2.weight.data[7][0] * 128), dtype = torch.int32)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_26236\\219235275.py:41: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  int_conv2_weight_82 =  torch.tensor((model.conv2.weight.data[7][1] * 128), dtype = torch.int32)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_26236\\219235275.py:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  int_conv2_weight_83 =  torch.tensor((model.conv2.weight.data[7][2] * 128), dtype = torch.int32)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_26236\\219235275.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  int_conv2_weight_84 =  torch.tensor((model.conv2.weight.data[7][3] * 128), dtype = torch.int32)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_26236\\219235275.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  int_conv2_weight_91 =  torch.tensor((model.conv2.weight.data[8][0] * 128), dtype = torch.int32)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_26236\\219235275.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  int_conv2_weight_92 =  torch.tensor((model.conv2.weight.data[8][1] * 128), dtype = torch.int32)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_26236\\219235275.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  int_conv2_weight_93 =  torch.tensor((model.conv2.weight.data[8][2] * 128), dtype = torch.int32)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_26236\\219235275.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  int_conv2_weight_94 =  torch.tensor((model.conv2.weight.data[8][3] * 128), dtype = torch.int32)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_26236\\219235275.py:50: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  int_conv2_weight_101 =  torch.tensor((model.conv2.weight.data[9][0] * 128), dtype = torch.int32)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_26236\\219235275.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  int_conv2_weight_102 =  torch.tensor((model.conv2.weight.data[9][1] * 128), dtype = torch.int32)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_26236\\219235275.py:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  int_conv2_weight_103 =  torch.tensor((model.conv2.weight.data[9][2] * 128), dtype = torch.int32)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_26236\\219235275.py:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  int_conv2_weight_104 =  torch.tensor((model.conv2.weight.data[9][3] * 128), dtype = torch.int32)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_26236\\219235275.py:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  int_conv2_weight_111 =  torch.tensor((model.conv2.weight.data[10][0] * 128), dtype = torch.int32)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_26236\\219235275.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  int_conv2_weight_112 =  torch.tensor((model.conv2.weight.data[10][1] * 128), dtype = torch.int32)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_26236\\219235275.py:57: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  int_conv2_weight_113 =  torch.tensor((model.conv2.weight.data[10][2] * 128), dtype = torch.int32)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_26236\\219235275.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  int_conv2_weight_114 =  torch.tensor((model.conv2.weight.data[10][3] * 128), dtype = torch.int32)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_26236\\219235275.py:60: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  int_conv2_weight_121 =  torch.tensor((model.conv2.weight.data[11][0] * 128), dtype = torch.int32)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_26236\\219235275.py:61: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  int_conv2_weight_122 =  torch.tensor((model.conv2.weight.data[11][1] * 128), dtype = torch.int32)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_26236\\219235275.py:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  int_conv2_weight_123 =  torch.tensor((model.conv2.weight.data[11][2] * 128), dtype = torch.int32)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_26236\\219235275.py:63: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  int_conv2_weight_124 =  torch.tensor((model.conv2.weight.data[11][3] * 128), dtype = torch.int32)\n"
     ]
    }
   ],
   "source": [
    "############## Conv2 ############\n",
    "import numpy as np\n",
    "print(np.shape(model.conv2.weight))\n",
    "\n",
    "int_conv2_weight_11 =  torch.tensor((model.conv2.weight.data[0][0]* 128), dtype = torch.int32)\n",
    "int_conv2_weight_12 =  torch.tensor((model.conv2.weight.data[0][1]* 128), dtype = torch.int32)\n",
    "int_conv2_weight_13 =  torch.tensor((model.conv2.weight.data[0][2]* 128), dtype = torch.int32)\n",
    "int_conv2_weight_14 =  torch.tensor((model.conv2.weight.data[0][3]* 128), dtype = torch.int32)\n",
    "\n",
    "int_conv2_weight_21 =  torch.tensor((model.conv2.weight.data[1][0] * 128), dtype = torch.int32)\n",
    "int_conv2_weight_22 =  torch.tensor((model.conv2.weight.data[1][1] * 128), dtype = torch.int32)\n",
    "int_conv2_weight_23 =  torch.tensor((model.conv2.weight.data[1][2] * 128), dtype = torch.int32)\n",
    "int_conv2_weight_24 =  torch.tensor((model.conv2.weight.data[1][3] * 128), dtype = torch.int32)\n",
    "\n",
    "int_conv2_weight_31 =  torch.tensor((model.conv2.weight.data[2][0] * 128), dtype = torch.int32)\n",
    "int_conv2_weight_32 =  torch.tensor((model.conv2.weight.data[2][1] * 128), dtype = torch.int32)\n",
    "int_conv2_weight_33 =  torch.tensor((model.conv2.weight.data[2][2] * 128), dtype = torch.int32)\n",
    "int_conv2_weight_34 =  torch.tensor((model.conv2.weight.data[2][3] * 128), dtype = torch.int32)\n",
    "\n",
    "int_conv2_weight_41 =  torch.tensor((model.conv2.weight.data[3][0] * 128), dtype = torch.int32)\n",
    "int_conv2_weight_42 =  torch.tensor((model.conv2.weight.data[3][1] * 128), dtype = torch.int32)\n",
    "int_conv2_weight_43 =  torch.tensor((model.conv2.weight.data[3][2] * 128), dtype = torch.int32)\n",
    "int_conv2_weight_44 =  torch.tensor((model.conv2.weight.data[3][3] * 128), dtype = torch.int32)\n",
    "\n",
    "int_conv2_weight_51 =  torch.tensor((model.conv2.weight.data[4][0] * 128), dtype = torch.int32)\n",
    "int_conv2_weight_52 =  torch.tensor((model.conv2.weight.data[4][1] * 128), dtype = torch.int32)\n",
    "int_conv2_weight_53 =  torch.tensor((model.conv2.weight.data[4][2] * 128), dtype = torch.int32)\n",
    "int_conv2_weight_54 =  torch.tensor((model.conv2.weight.data[4][3] * 128), dtype = torch.int32)\n",
    "\n",
    "int_conv2_weight_61 =  torch.tensor((model.conv2.weight.data[5][0] * 128), dtype = torch.int32)\n",
    "int_conv2_weight_62 =  torch.tensor((model.conv2.weight.data[5][1] * 128), dtype = torch.int32)\n",
    "int_conv2_weight_63 =  torch.tensor((model.conv2.weight.data[5][2] * 128), dtype = torch.int32)\n",
    "int_conv2_weight_64 =  torch.tensor((model.conv2.weight.data[5][3] * 128), dtype = torch.int32)\n",
    "\n",
    "int_conv2_weight_71 =  torch.tensor((model.conv2.weight.data[6][0] * 128), dtype = torch.int32)\n",
    "int_conv2_weight_72 =  torch.tensor((model.conv2.weight.data[6][1] * 128), dtype = torch.int32)\n",
    "int_conv2_weight_73 =  torch.tensor((model.conv2.weight.data[6][2] * 128), dtype = torch.int32)\n",
    "int_conv2_weight_74 =  torch.tensor((model.conv2.weight.data[6][3] * 128), dtype = torch.int32)\n",
    "\n",
    "int_conv2_weight_81 =  torch.tensor((model.conv2.weight.data[7][0] * 128), dtype = torch.int32)\n",
    "int_conv2_weight_82 =  torch.tensor((model.conv2.weight.data[7][1] * 128), dtype = torch.int32)\n",
    "int_conv2_weight_83 =  torch.tensor((model.conv2.weight.data[7][2] * 128), dtype = torch.int32)\n",
    "int_conv2_weight_84 =  torch.tensor((model.conv2.weight.data[7][3] * 128), dtype = torch.int32)\n",
    "\n",
    "int_conv2_weight_91 =  torch.tensor((model.conv2.weight.data[8][0] * 128), dtype = torch.int32)\n",
    "int_conv2_weight_92 =  torch.tensor((model.conv2.weight.data[8][1] * 128), dtype = torch.int32)\n",
    "int_conv2_weight_93 =  torch.tensor((model.conv2.weight.data[8][2] * 128), dtype = torch.int32)\n",
    "int_conv2_weight_94 =  torch.tensor((model.conv2.weight.data[8][3] * 128), dtype = torch.int32)\n",
    "\n",
    "int_conv2_weight_101 =  torch.tensor((model.conv2.weight.data[9][0] * 128), dtype = torch.int32)\n",
    "int_conv2_weight_102 =  torch.tensor((model.conv2.weight.data[9][1] * 128), dtype = torch.int32)\n",
    "int_conv2_weight_103 =  torch.tensor((model.conv2.weight.data[9][2] * 128), dtype = torch.int32)\n",
    "int_conv2_weight_104 =  torch.tensor((model.conv2.weight.data[9][3] * 128), dtype = torch.int32)\n",
    "\n",
    "int_conv2_weight_111 =  torch.tensor((model.conv2.weight.data[10][0] * 128), dtype = torch.int32)\n",
    "int_conv2_weight_112 =  torch.tensor((model.conv2.weight.data[10][1] * 128), dtype = torch.int32)\n",
    "int_conv2_weight_113 =  torch.tensor((model.conv2.weight.data[10][2] * 128), dtype = torch.int32)\n",
    "int_conv2_weight_114 =  torch.tensor((model.conv2.weight.data[10][3] * 128), dtype = torch.int32)\n",
    "\n",
    "int_conv2_weight_121 =  torch.tensor((model.conv2.weight.data[11][0] * 128), dtype = torch.int32)\n",
    "int_conv2_weight_122 =  torch.tensor((model.conv2.weight.data[11][1] * 128), dtype = torch.int32)\n",
    "int_conv2_weight_123 =  torch.tensor((model.conv2.weight.data[11][2] * 128), dtype = torch.int32)\n",
    "int_conv2_weight_124 =  torch.tensor((model.conv2.weight.data[11][3] * 128), dtype = torch.int32)\n",
    "\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        if int_conv2_weight_11[i][j] > 127:\n",
    "            int_conv2_weight_11[i][j] = 127\n",
    "        if int_conv2_weight_12[i][j] > 127:\n",
    "            int_conv2_weight_12[i][j] = 127\n",
    "        if int_conv2_weight_13[i][j] > 127:\n",
    "            int_conv2_weight_13[i][j] = 127\n",
    "        if int_conv2_weight_14[i][j] > 127:\n",
    "            int_conv2_weight_14[i][j] = 127\n",
    "            \n",
    "        if int_conv2_weight_21[i][j] > 127:\n",
    "            int_conv2_weight_21[i][j] = 127\n",
    "        if int_conv2_weight_22[i][j] > 127:\n",
    "            int_conv2_weight_22[i][j] = 127\n",
    "        if int_conv2_weight_23[i][j] > 127:\n",
    "            int_conv2_weight_23[i][j] = 127\n",
    "        if int_conv2_weight_24[i][j] > 127:\n",
    "            int_conv2_weight_24[i][j] = 127   \n",
    "             \n",
    "        if int_conv2_weight_31[i][j] > 127:\n",
    "            int_conv2_weight_31[i][j] = 127\n",
    "        if int_conv2_weight_32[i][j] > 127:\n",
    "            int_conv2_weight_32[i][j] = 127\n",
    "        if int_conv2_weight_33[i][j] > 127:\n",
    "            int_conv2_weight_33[i][j] = 127\n",
    "        if int_conv2_weight_34[i][j] > 127:\n",
    "            int_conv2_weight_34[i][j] = 127   \n",
    "        \n",
    "        if int_conv2_weight_41[i][j] > 127:\n",
    "            int_conv2_weight_41[i][j] = 127\n",
    "        if int_conv2_weight_42[i][j] > 127:\n",
    "            int_conv2_weight_42[i][j] = 127\n",
    "        if int_conv2_weight_43[i][j] > 127:\n",
    "            int_conv2_weight_43[i][j] = 127\n",
    "        if int_conv2_weight_44[i][j] > 127:\n",
    "            int_conv2_weight_44[i][j] = 127\n",
    "            \n",
    "        if int_conv2_weight_51[i][j] > 127:\n",
    "            int_conv2_weight_51[i][j] = 127\n",
    "        if int_conv2_weight_52[i][j] > 127:\n",
    "            int_conv2_weight_52[i][j] = 127\n",
    "        if int_conv2_weight_53[i][j] > 127:\n",
    "            int_conv2_weight_53[i][j] = 127\n",
    "        if int_conv2_weight_54[i][j] > 127:\n",
    "            int_conv2_weight_54[i][j] = 127   \n",
    "             \n",
    "        if int_conv2_weight_61[i][j] > 127:\n",
    "            int_conv2_weight_61[i][j] = 127\n",
    "        if int_conv2_weight_62[i][j] > 127:\n",
    "            int_conv2_weight_62[i][j] = 127\n",
    "        if int_conv2_weight_63[i][j] > 127:\n",
    "            int_conv2_weight_63[i][j] = 127\n",
    "        if int_conv2_weight_64[i][j] > 127:\n",
    "            int_conv2_weight_64[i][j] = 127\n",
    "        \n",
    "        if int_conv2_weight_71[i][j] > 127:\n",
    "            int_conv2_weight_71[i][j] = 127\n",
    "        if int_conv2_weight_72[i][j] > 127:\n",
    "            int_conv2_weight_72[i][j] = 127\n",
    "        if int_conv2_weight_73[i][j] > 127:\n",
    "            int_conv2_weight_73[i][j] = 127\n",
    "        if int_conv2_weight_74[i][j] > 127:\n",
    "            int_conv2_weight_74[i][j] = 127\n",
    "            \n",
    "        if int_conv2_weight_81[i][j] > 127:\n",
    "            int_conv2_weight_81[i][j] = 127\n",
    "        if int_conv2_weight_82[i][j] > 127:\n",
    "            int_conv2_weight_82[i][j] = 127\n",
    "        if int_conv2_weight_83[i][j] > 127:\n",
    "            int_conv2_weight_83[i][j] = 127\n",
    "        if int_conv2_weight_84[i][j] > 127:\n",
    "            int_conv2_weight_84[i][j] = 127   \n",
    "             \n",
    "        if int_conv2_weight_91[i][j] > 127:\n",
    "            int_conv2_weight_91[i][j] = 127\n",
    "        if int_conv2_weight_92[i][j] > 127:\n",
    "            int_conv2_weight_92[i][j] = 127\n",
    "        if int_conv2_weight_93[i][j] > 127:\n",
    "            int_conv2_weight_93[i][j] = 127\n",
    "        if int_conv2_weight_94[i][j] > 127:\n",
    "            int_conv2_weight_94[i][j] = 127  \n",
    "        \n",
    "        if int_conv2_weight_101[i][j] > 127:\n",
    "            int_conv2_weight_101[i][j] = 127\n",
    "        if int_conv2_weight_102[i][j] > 127:\n",
    "            int_conv2_weight_102[i][j] = 127\n",
    "        if int_conv2_weight_103[i][j] > 127:\n",
    "            int_conv2_weight_103[i][j] = 127\n",
    "        if int_conv2_weight_104[i][j] > 127:\n",
    "            int_conv2_weight_104[i][j] = 127\n",
    "            \n",
    "        if int_conv2_weight_111[i][j] > 127:\n",
    "            int_conv2_weight_111[i][j] = 127\n",
    "        if int_conv2_weight_112[i][j] > 127:\n",
    "            int_conv2_weight_112[i][j] = 127\n",
    "        if int_conv2_weight_113[i][j] > 127:\n",
    "            int_conv2_weight_113[i][j] = 127\n",
    "        if int_conv2_weight_114[i][j] > 127:\n",
    "            int_conv2_weight_114[i][j] = 127 \n",
    "             \n",
    "        if int_conv2_weight_121[i][j] > 127:\n",
    "            int_conv2_weight_121[i][j] = 127\n",
    "        if int_conv2_weight_122[i][j] > 127:\n",
    "            int_conv2_weight_122[i][j] = 127\n",
    "        if int_conv2_weight_123[i][j] > 127:\n",
    "            int_conv2_weight_123[i][j] = 127\n",
    "        if int_conv2_weight_124[i][j] > 127:\n",
    "            int_conv2_weight_124[i][j] = 127\n",
    "            \n",
    "int_conv2_weight_stack = np.vstack((int_conv2_weight_11,int_conv2_weight_21,int_conv2_weight_31,int_conv2_weight_41,int_conv2_weight_51,int_conv2_weight_61,\n",
    "                                        int_conv2_weight_71,int_conv2_weight_81,int_conv2_weight_91,int_conv2_weight_101,int_conv2_weight_111,int_conv2_weight_121,\n",
    "                                        int_conv2_weight_12,int_conv2_weight_22,int_conv2_weight_32,int_conv2_weight_42,int_conv2_weight_52,int_conv2_weight_62,\n",
    "                                        int_conv2_weight_72,int_conv2_weight_82,int_conv2_weight_92,int_conv2_weight_102,int_conv2_weight_112,int_conv2_weight_122,\n",
    "                                        int_conv2_weight_13,int_conv2_weight_23,int_conv2_weight_33,int_conv2_weight_43,int_conv2_weight_53,int_conv2_weight_63,\n",
    "                                        int_conv2_weight_73,int_conv2_weight_83,int_conv2_weight_93,int_conv2_weight_103,int_conv2_weight_113,int_conv2_weight_123,\n",
    "                                        int_conv2_weight_14,int_conv2_weight_24,int_conv2_weight_34,int_conv2_weight_44,int_conv2_weight_54,int_conv2_weight_64,\n",
    "                                        int_conv2_weight_74,int_conv2_weight_84,int_conv2_weight_94,int_conv2_weight_104,int_conv2_weight_114,int_conv2_weight_124))\n",
    "\n",
    "print (\"Signed\")\n",
    "print(int_conv2_weight_stack, '\\n')\n",
    "np.savetxt('SW_conv2_weight.txt', int_conv2_weight_stack, fmt='%d',delimiter = \" \")\n",
    "\n",
    "# signed int => unsigned int\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        if int_conv2_weight_11[i][j] < 0:\n",
    "            int_conv2_weight_11[i][j] += 256\n",
    "        if int_conv2_weight_12[i][j] < 0:\n",
    "            int_conv2_weight_12[i][j] += 256\n",
    "        if int_conv2_weight_13[i][j] < 0:\n",
    "            int_conv2_weight_13[i][j] += 256\n",
    "        if int_conv2_weight_14[i][j] < 0:\n",
    "            int_conv2_weight_14[i][j] += 256\n",
    "            \n",
    "        if int_conv2_weight_21[i][j] < 0:\n",
    "            int_conv2_weight_21[i][j] += 256\n",
    "        if int_conv2_weight_22[i][j] < 0:\n",
    "            int_conv2_weight_22[i][j] += 256\n",
    "        if int_conv2_weight_23[i][j] < 0:\n",
    "            int_conv2_weight_23[i][j] += 256\n",
    "        if int_conv2_weight_24[i][j] < 0:\n",
    "            int_conv2_weight_24[i][j] += 256   \n",
    "             \n",
    "        if int_conv2_weight_31[i][j] < 0:\n",
    "            int_conv2_weight_31[i][j] += 256\n",
    "        if int_conv2_weight_32[i][j] < 0:\n",
    "            int_conv2_weight_32[i][j] += 256\n",
    "        if int_conv2_weight_33[i][j] < 0:\n",
    "            int_conv2_weight_33[i][j] += 256\n",
    "        if int_conv2_weight_34[i][j] < 0:\n",
    "            int_conv2_weight_34[i][j] += 256   \n",
    "        \n",
    "        if int_conv2_weight_41[i][j] < 0:\n",
    "            int_conv2_weight_41[i][j] += 256\n",
    "        if int_conv2_weight_42[i][j] < 0:\n",
    "            int_conv2_weight_42[i][j] += 256\n",
    "        if int_conv2_weight_43[i][j] < 0:\n",
    "            int_conv2_weight_43[i][j] += 256\n",
    "        if int_conv2_weight_44[i][j] < 0:\n",
    "            int_conv2_weight_44[i][j] += 256\n",
    "            \n",
    "        if int_conv2_weight_51[i][j] < 0:\n",
    "            int_conv2_weight_51[i][j] += 256\n",
    "        if int_conv2_weight_52[i][j] < 0:\n",
    "            int_conv2_weight_52[i][j] += 256\n",
    "        if int_conv2_weight_53[i][j] < 0:\n",
    "            int_conv2_weight_53[i][j] += 256\n",
    "        if int_conv2_weight_54[i][j] < 0:\n",
    "            int_conv2_weight_54[i][j] += 256   \n",
    "             \n",
    "        if int_conv2_weight_61[i][j] < 0:\n",
    "            int_conv2_weight_61[i][j] += 256\n",
    "        if int_conv2_weight_62[i][j] < 0:\n",
    "            int_conv2_weight_62[i][j] += 256\n",
    "        if int_conv2_weight_63[i][j] < 0:\n",
    "            int_conv2_weight_63[i][j] += 256\n",
    "        if int_conv2_weight_64[i][j] < 0:\n",
    "            int_conv2_weight_64[i][j] += 256\n",
    "        \n",
    "        if int_conv2_weight_71[i][j] < 0:\n",
    "            int_conv2_weight_71[i][j] += 256\n",
    "        if int_conv2_weight_72[i][j] < 0:\n",
    "            int_conv2_weight_72[i][j] += 256\n",
    "        if int_conv2_weight_73[i][j] < 0:\n",
    "            int_conv2_weight_73[i][j] += 256\n",
    "        if int_conv2_weight_74[i][j] < 0:\n",
    "            int_conv2_weight_74[i][j] += 256\n",
    "            \n",
    "        if int_conv2_weight_81[i][j] < 0:\n",
    "            int_conv2_weight_81[i][j] += 256\n",
    "        if int_conv2_weight_82[i][j] < 0:\n",
    "            int_conv2_weight_82[i][j] += 256\n",
    "        if int_conv2_weight_83[i][j] < 0:\n",
    "            int_conv2_weight_83[i][j] += 256\n",
    "        if int_conv2_weight_84[i][j] < 0:\n",
    "            int_conv2_weight_84[i][j] += 256   \n",
    "             \n",
    "        if int_conv2_weight_91[i][j] < 0:\n",
    "            int_conv2_weight_91[i][j] += 256\n",
    "        if int_conv2_weight_92[i][j] < 0:\n",
    "            int_conv2_weight_92[i][j] += 256\n",
    "        if int_conv2_weight_93[i][j] < 0:\n",
    "            int_conv2_weight_93[i][j] += 256\n",
    "        if int_conv2_weight_94[i][j] < 0:\n",
    "            int_conv2_weight_94[i][j] += 256   \n",
    "        \n",
    "        if int_conv2_weight_101[i][j] < 0:\n",
    "            int_conv2_weight_101[i][j] += 256\n",
    "        if int_conv2_weight_102[i][j] < 0:\n",
    "            int_conv2_weight_102[i][j] += 256\n",
    "        if int_conv2_weight_103[i][j] < 0:\n",
    "            int_conv2_weight_103[i][j] += 256\n",
    "        if int_conv2_weight_104[i][j] < 0:\n",
    "            int_conv2_weight_104[i][j] += 256\n",
    "            \n",
    "        if int_conv2_weight_111[i][j] < 0:\n",
    "            int_conv2_weight_111[i][j] += 256\n",
    "        if int_conv2_weight_112[i][j] < 0:\n",
    "            int_conv2_weight_112[i][j] += 256\n",
    "        if int_conv2_weight_113[i][j] < 0:\n",
    "            int_conv2_weight_113[i][j] += 256\n",
    "        if int_conv2_weight_114[i][j] < 0:\n",
    "            int_conv2_weight_114[i][j] += 256   \n",
    "             \n",
    "        if int_conv2_weight_121[i][j] < 0:\n",
    "            int_conv2_weight_121[i][j] += 256\n",
    "        if int_conv2_weight_122[i][j] < 0:\n",
    "            int_conv2_weight_122[i][j] += 256\n",
    "        if int_conv2_weight_123[i][j] < 0:\n",
    "            int_conv2_weight_123[i][j] += 256\n",
    "        if int_conv2_weight_124[i][j] < 0:\n",
    "            int_conv2_weight_124[i][j] += 256   \n",
    "        \n",
    "\n",
    "int_conv2_weight_stack_ch1 = np.vstack((int_conv2_weight_11,int_conv2_weight_21,int_conv2_weight_31,int_conv2_weight_41,int_conv2_weight_51,int_conv2_weight_61,\n",
    "                                        int_conv2_weight_71,int_conv2_weight_81,int_conv2_weight_91,int_conv2_weight_101,int_conv2_weight_111,int_conv2_weight_121))\n",
    "int_conv2_weight_stack_ch2 = np.vstack((int_conv2_weight_12,int_conv2_weight_22,int_conv2_weight_32,int_conv2_weight_42,int_conv2_weight_52,int_conv2_weight_62,\n",
    "                                        int_conv2_weight_72,int_conv2_weight_82,int_conv2_weight_92,int_conv2_weight_102,int_conv2_weight_112,int_conv2_weight_122))\n",
    "int_conv2_weight_stack_ch3 = np.vstack((int_conv2_weight_13,int_conv2_weight_23,int_conv2_weight_33,int_conv2_weight_43,int_conv2_weight_53,int_conv2_weight_63,\n",
    "                                        int_conv2_weight_73,int_conv2_weight_83,int_conv2_weight_93,int_conv2_weight_103,int_conv2_weight_113,int_conv2_weight_123))\n",
    "int_conv2_weight_stack_ch4 = np.vstack((int_conv2_weight_14,int_conv2_weight_24,int_conv2_weight_34,int_conv2_weight_44,int_conv2_weight_54,int_conv2_weight_64,\n",
    "                                        int_conv2_weight_74,int_conv2_weight_84,int_conv2_weight_94,int_conv2_weight_104,int_conv2_weight_114,int_conv2_weight_124))\n",
    "\n",
    "print (\"Unsigned\")\n",
    "print(int_conv2_weight_stack_ch1, '\\n')\n",
    "print(int_conv2_weight_stack_ch2, '\\n')\n",
    "print(int_conv2_weight_stack_ch3, '\\n')\n",
    "print(int_conv2_weight_stack_ch4, '\\n')\n",
    "\n",
    "np.savetxt('HW_conv2_weight_ch1.mem', int_conv2_weight_stack_ch1, fmt='%1.2X',delimiter = \" \")\n",
    "np.savetxt('HW_conv2_weight_ch2.mem', int_conv2_weight_stack_ch2, fmt='%1.2X',delimiter = \" \")\n",
    "np.savetxt('HW_conv2_weight_ch3.mem', int_conv2_weight_stack_ch3, fmt='%1.2X',delimiter = \" \")\n",
    "np.savetxt('HW_conv2_weight_ch4.mem', int_conv2_weight_stack_ch4, fmt='%1.2X',delimiter = \" \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 192])\n",
      "tensor([[ -1, -11,   4,  ...,   0,  -5, -18],\n",
      "        [ 15,   4, -19,  ...,   9,  10,   8],\n",
      "        [  4,   0,  13,  ...,   6,   1,   7],\n",
      "        ...,\n",
      "        [  8,  10, -11,  ...,  -2,   5,   8],\n",
      "        [ -3,   4,   1,  ..., -10,  13,  11],\n",
      "        [-13,  -4,   4,  ...,   7,   5,  -3]], dtype=torch.int32)\n",
      "torch.Size([10])\n",
      "tensor([-2192,   134,  -447,   -71,  -915,  1531,  -650,  2332, -3347,   -28],\n",
      "       dtype=torch.int32)\n",
      "tensor([[255, 245,   4,  ...,   0, 251, 238],\n",
      "        [ 15,   4, 237,  ...,   9,  10,   8],\n",
      "        [  4,   0,  13,  ...,   6,   1,   7],\n",
      "        ...,\n",
      "        [  8,  10, 245,  ..., 254,   5,   8],\n",
      "        [253,   4,   1,  ..., 246,  13,  11],\n",
      "        [243, 252,   4,  ...,   7,   5, 253]], dtype=torch.int32)\n",
      "tensor([63344,   134, 65089, 65465, 64621,  1531, 64886,  2332, 62189, 65508],\n",
      "       dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "############## FC Layer ############\n",
    "\n",
    "print(np.shape(model.fc_1.weight))\n",
    "print((model.fc_1.weight * 128).int())\n",
    "\n",
    "print(np.shape(model.fc_1.bias))\n",
    "print((model.fc_1.bias * 128 * 256).int())\n",
    "\n",
    "int_fc_weight = (model.fc_1.weight * 128).int()\n",
    "int_fc_bias = (model.fc_1.bias * 128 * 256).int() #16bit bias\n",
    "for i in range(10):\n",
    "    for j in range(192):\n",
    "        if int_fc_weight[i][j] > 127 :\n",
    "            int_fc_weight[i][j] = 127\n",
    "            \n",
    "np.savetxt('SW_fc_weight.txt', int_fc_weight, fmt='%d',delimiter = \" \")\n",
    "np.savetxt('SW_fc_bias.txt', int_fc_bias, fmt='%d',delimiter = \" \")\n",
    "# signed int => unsigned int\n",
    "for i in range(10):\n",
    "    for j in range(192):\n",
    "        if int_fc_weight[i][j] < 0 :\n",
    "            int_fc_weight[i][j] += 256\n",
    "    if int_fc_bias[i] < 0 :\n",
    "        int_fc_bias[i] += 256*256 #16bit bias\n",
    "        \n",
    "print(int_fc_weight)\n",
    "print(int_fc_bias)\n",
    "\n",
    "np.savetxt('HW_fc_weight.mem', int_fc_weight, fmt='%1.2X',delimiter = \" \")\n",
    "np.savetxt('HW_fc_bias.mem', int_fc_bias, fmt='%1.2X',delimiter = \" \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "a077222d77dfe082b8f1dd562ad70e458ac2ab76993a0b248ab0476e32e9e8dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
